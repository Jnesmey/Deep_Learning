{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cee4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b784976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision as tv\n",
    "from torch import nn\n",
    "import torchvision.transforms as tfs\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88ea3e",
   "metadata": {},
   "source": [
    "Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c3a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Число бачей\n",
    "bs = 256\n",
    "# Преобразование\n",
    "transoforms = tv.transforms.Compose([\n",
    "    tv.transforms.Resize((224, 224)),\n",
    "    tv.transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8a42760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем данные\n",
    "# Указываю параметр split = 'balanced' для баланса\n",
    "\n",
    "# На этом совсем плохо обучалось\n",
    "\n",
    "# train_dataset = tv.datasets.EMNIST('.', train = True, transform=transoforms, download=True, split = 'balanced')\n",
    "# test_dataset = tv.datasets.EMNIST('.', train=False, transform=transoforms, download=True, split = 'balanced')\n",
    "# train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=bs)\n",
    "# test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d1771fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Второй вариант\n",
    "# Указываю параметр split = 'byclass' для большего количества данных\n",
    "train_dataset = tv.datasets.EMNIST('.', train = True, transform=transoforms, download=True, split = 'byclass')\n",
    "test_dataset = tv.datasets.EMNIST('.', train=False, transform=transoforms, download=True, split = 'byclass')\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=bs)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "637ac854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a59d032110>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Пример изображения\n",
    "plt.imshow(train_dataset[0][0].numpy().reshape(224,224), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90e92820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Число классов - 47 при балансе и 62 всего\n",
    "len(train_dataset.classes)\n",
    "# train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "145dc791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем девайс, который доступен\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c40f924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение функции обучения\n",
    "\n",
    "# Тут еще допишем сохранение результатов в словарь, \n",
    "#  что бы собрать таблицу для сравнения моделей\n",
    "\n",
    "def train(net, train_iter, test_iter, trainer, num_epochs):\n",
    "    d = {}\n",
    "    net.to(device)\n",
    "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        \n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            trainer.zero_grad()\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "\n",
    "            if i % 10 == 0:\n",
    "              print(f\"Step {i}. time since epoch: {time.time() -  start:.3f}. \" \n",
    "                    f\"Train acc: {train_acc_sum / n:.3f}. Train Loss: {train_l_sum / n:.3f}\")\n",
    "        test_acc = evaluate_accuracy(test_iter, net.to(device))\n",
    "        print('-' * 20)\n",
    "#         данные по эпохам записываем\n",
    "        d[epoch]: [train_l_sum / n, train_acc_sum / n, test_acc, time.time() - start]\n",
    "        print(f'epoch {epoch + 1}, loss {train_l_sum / n:.4f}, train acc {train_acc_sum / n:.3f}'\n",
    "              f', test acc {test_acc:.3f}, time {time.time() - start:.1f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d269ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Точность\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0, 0\n",
    "    net.eval()\n",
    "    for X, y in data_iter:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum.item() / n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "3480777d",
   "metadata": {},
   "source": [
    "Архитектура VGG16\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf336a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Прописываю все слои в модели\n",
    "# Не очень красиво, зато все наглядно и очевидно "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19862dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сеть по схеме с уменьшение в четыре раза\n",
    "model = nn.Sequential(\n",
    "# две свертки \n",
    "    nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "#     nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "#     nn.ReLU(),\n",
    "# пулинг и еще две свертки   \n",
    "    nn.MaxPool2d(2, stride=2),\n",
    "    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "#     nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "#     nn.ReLU(),\n",
    "# пулинг и три свертки\n",
    "    nn.MaxPool2d(2, stride=2),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "#     nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "#     nn.ReLU(),\n",
    "# пулинг и три свертки\n",
    "    nn.MaxPool2d(2, stride=2),\n",
    "    nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "#     nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "#     nn.ReLU(),\n",
    "# пулинг и три свертки  \n",
    "    nn.MaxPool2d(2, stride=2),\n",
    "    nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "#     nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "#     nn.ReLU(),   \n",
    "# последний пулинг\n",
    "    nn.MaxPool2d(2, stride=2),\n",
    "    nn.Flatten(),\n",
    "# # линейные слои\n",
    "    nn.Linear(6272, 4096),\n",
    "    nn.ReLU(), \n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(4096, 4096), \n",
    "    nn.ReLU(), \n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(4096, 62)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3afc9f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 224, 224]             160\n",
      "              ReLU-2         [-1, 16, 224, 224]               0\n",
      "         MaxPool2d-3         [-1, 16, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]           4,640\n",
      "              ReLU-5         [-1, 32, 112, 112]               0\n",
      "         MaxPool2d-6           [-1, 32, 56, 56]               0\n",
      "            Conv2d-7           [-1, 64, 56, 56]          18,496\n",
      "              ReLU-8           [-1, 64, 56, 56]               0\n",
      "            Conv2d-9           [-1, 64, 56, 56]          36,928\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "        MaxPool2d-11           [-1, 64, 28, 28]               0\n",
      "           Conv2d-12          [-1, 128, 28, 28]          73,856\n",
      "             ReLU-13          [-1, 128, 28, 28]               0\n",
      "           Conv2d-14          [-1, 128, 28, 28]         147,584\n",
      "             ReLU-15          [-1, 128, 28, 28]               0\n",
      "        MaxPool2d-16          [-1, 128, 14, 14]               0\n",
      "           Conv2d-17          [-1, 128, 14, 14]         147,584\n",
      "             ReLU-18          [-1, 128, 14, 14]               0\n",
      "           Conv2d-19          [-1, 128, 14, 14]         147,584\n",
      "             ReLU-20          [-1, 128, 14, 14]               0\n",
      "        MaxPool2d-21            [-1, 128, 7, 7]               0\n",
      "          Flatten-22                 [-1, 6272]               0\n",
      "           Linear-23                 [-1, 4096]      25,694,208\n",
      "             ReLU-24                 [-1, 4096]               0\n",
      "          Dropout-25                 [-1, 4096]               0\n",
      "           Linear-26                 [-1, 4096]      16,781,312\n",
      "             ReLU-27                 [-1, 4096]               0\n",
      "          Dropout-28                 [-1, 4096]               0\n",
      "           Linear-29                   [-1, 62]         254,014\n",
      "================================================================\n",
      "Total params: 43,306,366\n",
      "Trainable params: 43,306,366\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 31.48\n",
      "Params size (MB): 165.20\n",
      "Estimated Total Size (MB): 196.87\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model.to(device), input_size=(1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8941561e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0. time since epoch: 6.111. Train acc: 0.012. Train Loss: 4.128\n",
      "Step 10. time since epoch: 57.802. Train acc: 0.042. Train Loss: 3.896\n",
      "Step 20. time since epoch: 107.786. Train acc: 0.045. Train Loss: 3.806\n",
      "Step 30. time since epoch: 157.612. Train acc: 0.045. Train Loss: 3.778\n",
      "Step 40. time since epoch: 207.064. Train acc: 0.045. Train Loss: 3.765\n",
      "Step 50. time since epoch: 257.033. Train acc: 0.047. Train Loss: 3.757\n",
      "Step 60. time since epoch: 305.470. Train acc: 0.047. Train Loss: 3.750\n",
      "Step 70. time since epoch: 355.778. Train acc: 0.048. Train Loss: 3.740\n",
      "Step 80. time since epoch: 404.795. Train acc: 0.048. Train Loss: 3.734\n",
      "Step 90. time since epoch: 453.636. Train acc: 0.048. Train Loss: 3.729\n",
      "Step 100. time since epoch: 503.999. Train acc: 0.048. Train Loss: 3.727\n",
      "Step 110. time since epoch: 553.102. Train acc: 0.047. Train Loss: 3.726\n",
      "Step 120. time since epoch: 601.717. Train acc: 0.047. Train Loss: 3.723\n",
      "Step 130. time since epoch: 650.032. Train acc: 0.047. Train Loss: 3.720\n",
      "Step 140. time since epoch: 700.675. Train acc: 0.047. Train Loss: 3.720\n",
      "Step 150. time since epoch: 749.905. Train acc: 0.047. Train Loss: 3.718\n",
      "Step 160. time since epoch: 800.117. Train acc: 0.047. Train Loss: 3.713\n",
      "Step 170. time since epoch: 850.198. Train acc: 0.053. Train Loss: 3.695\n",
      "Step 180. time since epoch: 899.590. Train acc: 0.069. Train Loss: 3.639\n",
      "Step 190. time since epoch: 950.400. Train acc: 0.090. Train Loss: 3.557\n",
      "Step 200. time since epoch: 998.871. Train acc: 0.113. Train Loss: 3.461\n",
      "Step 210. time since epoch: 1047.075. Train acc: 0.136. Train Loss: 3.361\n",
      "Step 220. time since epoch: 1095.264. Train acc: 0.159. Train Loss: 3.265\n",
      "Step 230. time since epoch: 1143.481. Train acc: 0.181. Train Loss: 3.170\n",
      "Step 240. time since epoch: 1192.109. Train acc: 0.201. Train Loss: 3.083\n",
      "Step 250. time since epoch: 1240.873. Train acc: 0.221. Train Loss: 2.998\n",
      "Step 260. time since epoch: 1289.551. Train acc: 0.241. Train Loss: 2.916\n",
      "Step 270. time since epoch: 1337.879. Train acc: 0.259. Train Loss: 2.840\n",
      "Step 280. time since epoch: 1386.087. Train acc: 0.277. Train Loss: 2.767\n",
      "Step 290. time since epoch: 1435.238. Train acc: 0.293. Train Loss: 2.698\n",
      "Step 300. time since epoch: 1483.394. Train acc: 0.309. Train Loss: 2.633\n",
      "Step 310. time since epoch: 1532.724. Train acc: 0.323. Train Loss: 2.574\n",
      "Step 320. time since epoch: 1581.477. Train acc: 0.337. Train Loss: 2.516\n",
      "Step 330. time since epoch: 1630.095. Train acc: 0.350. Train Loss: 2.460\n",
      "Step 340. time since epoch: 1678.931. Train acc: 0.363. Train Loss: 2.409\n",
      "Step 350. time since epoch: 1727.841. Train acc: 0.374. Train Loss: 2.360\n",
      "Step 360. time since epoch: 1776.592. Train acc: 0.385. Train Loss: 2.314\n",
      "Step 370. time since epoch: 1824.890. Train acc: 0.396. Train Loss: 2.269\n",
      "Step 380. time since epoch: 1873.504. Train acc: 0.406. Train Loss: 2.227\n",
      "Step 390. time since epoch: 1921.638. Train acc: 0.416. Train Loss: 2.186\n",
      "Step 400. time since epoch: 1970.820. Train acc: 0.425. Train Loss: 2.147\n",
      "Step 410. time since epoch: 2019.353. Train acc: 0.434. Train Loss: 2.109\n",
      "Step 420. time since epoch: 2067.945. Train acc: 0.442. Train Loss: 2.074\n",
      "Step 430. time since epoch: 2116.852. Train acc: 0.451. Train Loss: 2.039\n",
      "Step 440. time since epoch: 2165.394. Train acc: 0.459. Train Loss: 2.007\n",
      "Step 450. time since epoch: 2213.585. Train acc: 0.466. Train Loss: 1.976\n",
      "Step 460. time since epoch: 2261.240. Train acc: 0.474. Train Loss: 1.946\n",
      "Step 470. time since epoch: 2309.086. Train acc: 0.481. Train Loss: 1.916\n",
      "Step 480. time since epoch: 2358.514. Train acc: 0.488. Train Loss: 1.887\n",
      "Step 490. time since epoch: 2406.945. Train acc: 0.494. Train Loss: 1.860\n",
      "Step 500. time since epoch: 2455.571. Train acc: 0.501. Train Loss: 1.834\n",
      "Step 510. time since epoch: 2504.291. Train acc: 0.507. Train Loss: 1.809\n",
      "Step 520. time since epoch: 2552.407. Train acc: 0.513. Train Loss: 1.786\n",
      "Step 530. time since epoch: 2600.584. Train acc: 0.518. Train Loss: 1.762\n",
      "Step 540. time since epoch: 2649.125. Train acc: 0.524. Train Loss: 1.739\n",
      "Step 550. time since epoch: 2697.374. Train acc: 0.529. Train Loss: 1.717\n",
      "Step 560. time since epoch: 2745.548. Train acc: 0.534. Train Loss: 1.697\n",
      "Step 570. time since epoch: 2793.715. Train acc: 0.539. Train Loss: 1.676\n",
      "Step 580. time since epoch: 2842.241. Train acc: 0.544. Train Loss: 1.657\n",
      "Step 590. time since epoch: 2890.954. Train acc: 0.548. Train Loss: 1.638\n",
      "Step 600. time since epoch: 2939.523. Train acc: 0.553. Train Loss: 1.620\n",
      "Step 610. time since epoch: 2989.268. Train acc: 0.557. Train Loss: 1.602\n",
      "Step 620. time since epoch: 3037.963. Train acc: 0.561. Train Loss: 1.584\n",
      "Step 630. time since epoch: 3086.106. Train acc: 0.566. Train Loss: 1.568\n",
      "Step 640. time since epoch: 3133.973. Train acc: 0.570. Train Loss: 1.551\n",
      "Step 650. time since epoch: 3182.433. Train acc: 0.573. Train Loss: 1.535\n",
      "Step 660. time since epoch: 3230.433. Train acc: 0.577. Train Loss: 1.519\n",
      "Step 670. time since epoch: 3278.935. Train acc: 0.581. Train Loss: 1.504\n",
      "Step 680. time since epoch: 3327.153. Train acc: 0.585. Train Loss: 1.490\n",
      "Step 690. time since epoch: 3375.437. Train acc: 0.588. Train Loss: 1.476\n",
      "Step 700. time since epoch: 3423.732. Train acc: 0.592. Train Loss: 1.462\n",
      "Step 710. time since epoch: 3471.742. Train acc: 0.595. Train Loss: 1.449\n",
      "Step 720. time since epoch: 3519.644. Train acc: 0.598. Train Loss: 1.436\n",
      "Step 730. time since epoch: 3567.787. Train acc: 0.601. Train Loss: 1.424\n",
      "Step 740. time since epoch: 3615.424. Train acc: 0.604. Train Loss: 1.411\n",
      "Step 750. time since epoch: 3663.957. Train acc: 0.607. Train Loss: 1.399\n",
      "Step 760. time since epoch: 3712.108. Train acc: 0.610. Train Loss: 1.387\n",
      "Step 770. time since epoch: 3759.597. Train acc: 0.613. Train Loss: 1.375\n",
      "Step 780. time since epoch: 3807.837. Train acc: 0.616. Train Loss: 1.364\n",
      "Step 790. time since epoch: 3855.630. Train acc: 0.619. Train Loss: 1.353\n",
      "Step 800. time since epoch: 3903.723. Train acc: 0.621. Train Loss: 1.342\n",
      "Step 810. time since epoch: 3955.718. Train acc: 0.624. Train Loss: 1.332\n",
      "Step 820. time since epoch: 4009.015. Train acc: 0.626. Train Loss: 1.321\n",
      "Step 830. time since epoch: 4061.448. Train acc: 0.629. Train Loss: 1.311\n",
      "Step 840. time since epoch: 4113.435. Train acc: 0.631. Train Loss: 1.302\n",
      "Step 850. time since epoch: 4167.507. Train acc: 0.634. Train Loss: 1.292\n",
      "Step 860. time since epoch: 4221.305. Train acc: 0.636. Train Loss: 1.283\n",
      "Step 870. time since epoch: 4274.615. Train acc: 0.638. Train Loss: 1.273\n",
      "Step 880. time since epoch: 4327.662. Train acc: 0.640. Train Loss: 1.265\n",
      "Step 890. time since epoch: 4382.542. Train acc: 0.643. Train Loss: 1.256\n",
      "Step 900. time since epoch: 4435.765. Train acc: 0.645. Train Loss: 1.247\n",
      "Step 910. time since epoch: 4489.478. Train acc: 0.647. Train Loss: 1.239\n",
      "Step 920. time since epoch: 4540.838. Train acc: 0.649. Train Loss: 1.231\n",
      "Step 930. time since epoch: 4590.104. Train acc: 0.651. Train Loss: 1.223\n",
      "Step 940. time since epoch: 4641.352. Train acc: 0.653. Train Loss: 1.214\n",
      "Step 950. time since epoch: 4694.226. Train acc: 0.655. Train Loss: 1.206\n",
      "Step 960. time since epoch: 4742.897. Train acc: 0.657. Train Loss: 1.199\n",
      "Step 970. time since epoch: 4791.630. Train acc: 0.659. Train Loss: 1.191\n",
      "Step 980. time since epoch: 4841.831. Train acc: 0.660. Train Loss: 1.184\n",
      "Step 990. time since epoch: 4891.611. Train acc: 0.662. Train Loss: 1.177\n",
      "Step 1000. time since epoch: 4954.645. Train acc: 0.664. Train Loss: 1.170\n",
      "Step 1010. time since epoch: 5011.367. Train acc: 0.666. Train Loss: 1.163\n",
      "Step 1020. time since epoch: 5069.558. Train acc: 0.667. Train Loss: 1.156\n",
      "Step 1030. time since epoch: 5131.862. Train acc: 0.669. Train Loss: 1.149\n",
      "Step 1040. time since epoch: 5191.710. Train acc: 0.671. Train Loss: 1.143\n",
      "Step 1050. time since epoch: 5250.832. Train acc: 0.672. Train Loss: 1.136\n",
      "Step 1060. time since epoch: 5307.532. Train acc: 0.674. Train Loss: 1.130\n",
      "Step 1070. time since epoch: 5362.976. Train acc: 0.675. Train Loss: 1.124\n",
      "Step 1080. time since epoch: 5413.043. Train acc: 0.677. Train Loss: 1.118\n",
      "Step 1090. time since epoch: 5465.383. Train acc: 0.678. Train Loss: 1.112\n",
      "Step 1100. time since epoch: 5513.769. Train acc: 0.680. Train Loss: 1.106\n",
      "Step 1110. time since epoch: 5562.973. Train acc: 0.681. Train Loss: 1.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1120. time since epoch: 5614.087. Train acc: 0.683. Train Loss: 1.094\n",
      "Step 1130. time since epoch: 5667.272. Train acc: 0.684. Train Loss: 1.089\n",
      "Step 1140. time since epoch: 5721.203. Train acc: 0.685. Train Loss: 1.084\n",
      "Step 1150. time since epoch: 5774.716. Train acc: 0.687. Train Loss: 1.078\n",
      "Step 1160. time since epoch: 5826.532. Train acc: 0.688. Train Loss: 1.073\n",
      "Step 1170. time since epoch: 5876.419. Train acc: 0.689. Train Loss: 1.068\n",
      "Step 1180. time since epoch: 5927.700. Train acc: 0.690. Train Loss: 1.062\n",
      "Step 1190. time since epoch: 5978.988. Train acc: 0.692. Train Loss: 1.058\n",
      "Step 1200. time since epoch: 6028.543. Train acc: 0.693. Train Loss: 1.053\n",
      "Step 1210. time since epoch: 6078.606. Train acc: 0.694. Train Loss: 1.048\n",
      "Step 1220. time since epoch: 6127.228. Train acc: 0.695. Train Loss: 1.043\n",
      "Step 1230. time since epoch: 6178.254. Train acc: 0.697. Train Loss: 1.038\n",
      "Step 1240. time since epoch: 6229.971. Train acc: 0.698. Train Loss: 1.033\n",
      "Step 1250. time since epoch: 6282.508. Train acc: 0.699. Train Loss: 1.028\n",
      "Step 1260. time since epoch: 6334.972. Train acc: 0.700. Train Loss: 1.024\n",
      "Step 1270. time since epoch: 6387.206. Train acc: 0.701. Train Loss: 1.019\n",
      "Step 1280. time since epoch: 6437.691. Train acc: 0.702. Train Loss: 1.015\n",
      "Step 1290. time since epoch: 6486.857. Train acc: 0.703. Train Loss: 1.011\n",
      "Step 1300. time since epoch: 6536.201. Train acc: 0.704. Train Loss: 1.006\n",
      "Step 1310. time since epoch: 6585.692. Train acc: 0.705. Train Loss: 1.002\n",
      "Step 1320. time since epoch: 6637.955. Train acc: 0.706. Train Loss: 0.998\n",
      "Step 1330. time since epoch: 6697.697. Train acc: 0.707. Train Loss: 0.994\n",
      "Step 1340. time since epoch: 6750.089. Train acc: 0.708. Train Loss: 0.989\n",
      "Step 1350. time since epoch: 6807.952. Train acc: 0.709. Train Loss: 0.985\n",
      "Step 1360. time since epoch: 6864.896. Train acc: 0.710. Train Loss: 0.982\n",
      "Step 1370. time since epoch: 6919.244. Train acc: 0.711. Train Loss: 0.978\n",
      "Step 1380. time since epoch: 6974.255. Train acc: 0.712. Train Loss: 0.974\n",
      "Step 1390. time since epoch: 7028.274. Train acc: 0.713. Train Loss: 0.970\n",
      "Step 1400. time since epoch: 7083.680. Train acc: 0.714. Train Loss: 0.966\n",
      "Step 1410. time since epoch: 7136.269. Train acc: 0.715. Train Loss: 0.963\n",
      "Step 1420. time since epoch: 7189.905. Train acc: 0.716. Train Loss: 0.959\n",
      "Step 1430. time since epoch: 7242.985. Train acc: 0.717. Train Loss: 0.955\n",
      "Step 1440. time since epoch: 7295.866. Train acc: 0.718. Train Loss: 0.952\n",
      "Step 1450. time since epoch: 7347.551. Train acc: 0.718. Train Loss: 0.948\n",
      "Step 1460. time since epoch: 7399.460. Train acc: 0.719. Train Loss: 0.945\n",
      "Step 1470. time since epoch: 7451.966. Train acc: 0.720. Train Loss: 0.942\n",
      "Step 1480. time since epoch: 7502.946. Train acc: 0.721. Train Loss: 0.938\n",
      "Step 1490. time since epoch: 7554.941. Train acc: 0.722. Train Loss: 0.935\n",
      "Step 1500. time since epoch: 7606.531. Train acc: 0.723. Train Loss: 0.932\n",
      "Step 1510. time since epoch: 7655.376. Train acc: 0.723. Train Loss: 0.928\n",
      "Step 1520. time since epoch: 7704.853. Train acc: 0.724. Train Loss: 0.925\n",
      "Step 1530. time since epoch: 7754.592. Train acc: 0.725. Train Loss: 0.922\n",
      "Step 1540. time since epoch: 7804.520. Train acc: 0.726. Train Loss: 0.919\n",
      "Step 1550. time since epoch: 7853.289. Train acc: 0.727. Train Loss: 0.916\n",
      "Step 1560. time since epoch: 7902.048. Train acc: 0.727. Train Loss: 0.913\n",
      "Step 1570. time since epoch: 7950.923. Train acc: 0.728. Train Loss: 0.910\n",
      "Step 1580. time since epoch: 7999.946. Train acc: 0.729. Train Loss: 0.907\n",
      "Step 1590. time since epoch: 8048.512. Train acc: 0.730. Train Loss: 0.904\n",
      "Step 1600. time since epoch: 8097.270. Train acc: 0.730. Train Loss: 0.902\n",
      "Step 1610. time since epoch: 8146.264. Train acc: 0.731. Train Loss: 0.899\n",
      "Step 1620. time since epoch: 8217.207. Train acc: 0.732. Train Loss: 0.896\n",
      "Step 1630. time since epoch: 8291.602. Train acc: 0.733. Train Loss: 0.893\n",
      "Step 1640. time since epoch: 8353.393. Train acc: 0.733. Train Loss: 0.890\n",
      "Step 1650. time since epoch: 8419.182. Train acc: 0.734. Train Loss: 0.887\n",
      "Step 1660. time since epoch: 8474.629. Train acc: 0.735. Train Loss: 0.884\n",
      "Step 1670. time since epoch: 8526.507. Train acc: 0.735. Train Loss: 0.882\n",
      "Step 1680. time since epoch: 8581.064. Train acc: 0.736. Train Loss: 0.879\n",
      "Step 1690. time since epoch: 8633.698. Train acc: 0.736. Train Loss: 0.876\n",
      "Step 1700. time since epoch: 8683.557. Train acc: 0.737. Train Loss: 0.874\n",
      "Step 1710. time since epoch: 8734.600. Train acc: 0.738. Train Loss: 0.871\n",
      "Step 1720. time since epoch: 8784.181. Train acc: 0.738. Train Loss: 0.869\n",
      "Step 1730. time since epoch: 8834.246. Train acc: 0.739. Train Loss: 0.866\n",
      "Step 1740. time since epoch: 8885.670. Train acc: 0.740. Train Loss: 0.864\n",
      "Step 1750. time since epoch: 8936.471. Train acc: 0.740. Train Loss: 0.861\n",
      "Step 1760. time since epoch: 8991.423. Train acc: 0.741. Train Loss: 0.859\n",
      "Step 1770. time since epoch: 9049.804. Train acc: 0.741. Train Loss: 0.857\n",
      "Step 1780. time since epoch: 9104.539. Train acc: 0.742. Train Loss: 0.854\n",
      "Step 1790. time since epoch: 9161.302. Train acc: 0.742. Train Loss: 0.852\n",
      "Step 1800. time since epoch: 9215.428. Train acc: 0.743. Train Loss: 0.850\n",
      "Step 1810. time since epoch: 9267.966. Train acc: 0.743. Train Loss: 0.848\n",
      "Step 1820. time since epoch: 9322.472. Train acc: 0.744. Train Loss: 0.845\n",
      "Step 1830. time since epoch: 9382.481. Train acc: 0.745. Train Loss: 0.843\n",
      "Step 1840. time since epoch: 9437.628. Train acc: 0.745. Train Loss: 0.841\n",
      "Step 1850. time since epoch: 9494.961. Train acc: 0.746. Train Loss: 0.839\n",
      "Step 1860. time since epoch: 9551.769. Train acc: 0.746. Train Loss: 0.837\n",
      "Step 1870. time since epoch: 9608.007. Train acc: 0.747. Train Loss: 0.835\n",
      "Step 1880. time since epoch: 9665.427. Train acc: 0.747. Train Loss: 0.832\n",
      "Step 1890. time since epoch: 9719.709. Train acc: 0.748. Train Loss: 0.830\n",
      "Step 1900. time since epoch: 9767.716. Train acc: 0.748. Train Loss: 0.828\n",
      "Step 1910. time since epoch: 9825.963. Train acc: 0.749. Train Loss: 0.826\n",
      "Step 1920. time since epoch: 9884.270. Train acc: 0.749. Train Loss: 0.824\n",
      "Step 1930. time since epoch: 9938.157. Train acc: 0.750. Train Loss: 0.822\n",
      "Step 1940. time since epoch: 9998.460. Train acc: 0.751. Train Loss: 0.820\n",
      "Step 1950. time since epoch: 10056.985. Train acc: 0.751. Train Loss: 0.818\n",
      "Step 1960. time since epoch: 10110.757. Train acc: 0.751. Train Loss: 0.816\n",
      "Step 1970. time since epoch: 10164.821. Train acc: 0.752. Train Loss: 0.814\n",
      "Step 1980. time since epoch: 10225.993. Train acc: 0.752. Train Loss: 0.812\n",
      "Step 1990. time since epoch: 10284.078. Train acc: 0.753. Train Loss: 0.810\n",
      "Step 2000. time since epoch: 10344.797. Train acc: 0.753. Train Loss: 0.808\n",
      "Step 2010. time since epoch: 10400.864. Train acc: 0.754. Train Loss: 0.806\n",
      "Step 2020. time since epoch: 10452.010. Train acc: 0.754. Train Loss: 0.804\n",
      "Step 2030. time since epoch: 10508.608. Train acc: 0.755. Train Loss: 0.802\n",
      "Step 2040. time since epoch: 10557.651. Train acc: 0.755. Train Loss: 0.800\n",
      "Step 2050. time since epoch: 10607.765. Train acc: 0.756. Train Loss: 0.798\n",
      "Step 2060. time since epoch: 10658.127. Train acc: 0.756. Train Loss: 0.796\n",
      "Step 2070. time since epoch: 10710.540. Train acc: 0.757. Train Loss: 0.794\n",
      "Step 2080. time since epoch: 10761.297. Train acc: 0.757. Train Loss: 0.793\n",
      "Step 2090. time since epoch: 10812.972. Train acc: 0.758. Train Loss: 0.791\n",
      "Step 2100. time since epoch: 10864.873. Train acc: 0.758. Train Loss: 0.789\n",
      "Step 2110. time since epoch: 10916.802. Train acc: 0.759. Train Loss: 0.787\n",
      "Step 2120. time since epoch: 10968.430. Train acc: 0.759. Train Loss: 0.785\n",
      "Step 2130. time since epoch: 11019.609. Train acc: 0.759. Train Loss: 0.784\n",
      "Step 2140. time since epoch: 11071.246. Train acc: 0.760. Train Loss: 0.782\n",
      "Step 2150. time since epoch: 11121.652. Train acc: 0.760. Train Loss: 0.780\n",
      "Step 2160. time since epoch: 11173.490. Train acc: 0.761. Train Loss: 0.778\n",
      "Step 2170. time since epoch: 11224.464. Train acc: 0.761. Train Loss: 0.777\n",
      "Step 2180. time since epoch: 11276.147. Train acc: 0.762. Train Loss: 0.775\n",
      "Step 2190. time since epoch: 11326.135. Train acc: 0.762. Train Loss: 0.774\n",
      "Step 2200. time since epoch: 11377.965. Train acc: 0.762. Train Loss: 0.772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2210. time since epoch: 11430.546. Train acc: 0.763. Train Loss: 0.771\n",
      "Step 2220. time since epoch: 11480.770. Train acc: 0.763. Train Loss: 0.769\n",
      "Step 2230. time since epoch: 11531.346. Train acc: 0.763. Train Loss: 0.768\n",
      "Step 2240. time since epoch: 11583.469. Train acc: 0.764. Train Loss: 0.766\n",
      "Step 2250. time since epoch: 11633.644. Train acc: 0.764. Train Loss: 0.765\n",
      "Step 2260. time since epoch: 11685.701. Train acc: 0.764. Train Loss: 0.763\n",
      "Step 2270. time since epoch: 11736.228. Train acc: 0.765. Train Loss: 0.762\n",
      "Step 2280. time since epoch: 11788.690. Train acc: 0.765. Train Loss: 0.760\n",
      "Step 2290. time since epoch: 11838.699. Train acc: 0.766. Train Loss: 0.759\n",
      "Step 2300. time since epoch: 11889.395. Train acc: 0.766. Train Loss: 0.757\n",
      "Step 2310. time since epoch: 11940.292. Train acc: 0.766. Train Loss: 0.755\n",
      "Step 2320. time since epoch: 11992.119. Train acc: 0.767. Train Loss: 0.754\n",
      "Step 2330. time since epoch: 12044.653. Train acc: 0.767. Train Loss: 0.753\n",
      "Step 2340. time since epoch: 12096.355. Train acc: 0.767. Train Loss: 0.751\n",
      "Step 2350. time since epoch: 12146.695. Train acc: 0.768. Train Loss: 0.750\n",
      "Step 2360. time since epoch: 12199.583. Train acc: 0.768. Train Loss: 0.748\n",
      "Step 2370. time since epoch: 12247.942. Train acc: 0.768. Train Loss: 0.747\n",
      "Step 2380. time since epoch: 12297.154. Train acc: 0.769. Train Loss: 0.745\n",
      "Step 2390. time since epoch: 12346.643. Train acc: 0.769. Train Loss: 0.744\n",
      "Step 2400. time since epoch: 12395.810. Train acc: 0.769. Train Loss: 0.743\n",
      "Step 2410. time since epoch: 12446.551. Train acc: 0.770. Train Loss: 0.741\n",
      "Step 2420. time since epoch: 12497.762. Train acc: 0.770. Train Loss: 0.740\n",
      "Step 2430. time since epoch: 12550.366. Train acc: 0.771. Train Loss: 0.739\n",
      "Step 2440. time since epoch: 12609.866. Train acc: 0.771. Train Loss: 0.737\n",
      "Step 2450. time since epoch: 12663.470. Train acc: 0.771. Train Loss: 0.736\n",
      "Step 2460. time since epoch: 12718.055. Train acc: 0.772. Train Loss: 0.735\n",
      "Step 2470. time since epoch: 12771.290. Train acc: 0.772. Train Loss: 0.733\n",
      "Step 2480. time since epoch: 12823.792. Train acc: 0.772. Train Loss: 0.732\n",
      "Step 2490. time since epoch: 12875.159. Train acc: 0.772. Train Loss: 0.731\n",
      "Step 2500. time since epoch: 12926.089. Train acc: 0.773. Train Loss: 0.729\n",
      "Step 2510. time since epoch: 12975.792. Train acc: 0.773. Train Loss: 0.728\n",
      "Step 2520. time since epoch: 13025.805. Train acc: 0.773. Train Loss: 0.727\n",
      "Step 2530. time since epoch: 13075.894. Train acc: 0.774. Train Loss: 0.726\n",
      "Step 2540. time since epoch: 13126.911. Train acc: 0.774. Train Loss: 0.724\n",
      "Step 2550. time since epoch: 13179.034. Train acc: 0.774. Train Loss: 0.723\n",
      "Step 2560. time since epoch: 13229.331. Train acc: 0.775. Train Loss: 0.722\n",
      "Step 2570. time since epoch: 13281.387. Train acc: 0.775. Train Loss: 0.721\n",
      "Step 2580. time since epoch: 13334.302. Train acc: 0.775. Train Loss: 0.720\n",
      "Step 2590. time since epoch: 13388.951. Train acc: 0.775. Train Loss: 0.719\n",
      "Step 2600. time since epoch: 13444.331. Train acc: 0.776. Train Loss: 0.717\n",
      "Step 2610. time since epoch: 13499.892. Train acc: 0.776. Train Loss: 0.716\n",
      "Step 2620. time since epoch: 13553.996. Train acc: 0.776. Train Loss: 0.715\n",
      "Step 2630. time since epoch: 13607.538. Train acc: 0.777. Train Loss: 0.714\n",
      "Step 2640. time since epoch: 13661.805. Train acc: 0.777. Train Loss: 0.713\n",
      "Step 2650. time since epoch: 13716.355. Train acc: 0.777. Train Loss: 0.711\n",
      "Step 2660. time since epoch: 13767.030. Train acc: 0.777. Train Loss: 0.710\n",
      "Step 2670. time since epoch: 13818.550. Train acc: 0.778. Train Loss: 0.709\n",
      "Step 2680. time since epoch: 13870.463. Train acc: 0.778. Train Loss: 0.708\n",
      "Step 2690. time since epoch: 13923.249. Train acc: 0.778. Train Loss: 0.707\n",
      "Step 2700. time since epoch: 13976.050. Train acc: 0.779. Train Loss: 0.706\n",
      "Step 2710. time since epoch: 14041.220. Train acc: 0.779. Train Loss: 0.705\n",
      "Step 2720. time since epoch: 14100.616. Train acc: 0.779. Train Loss: 0.704\n",
      "--------------------\n",
      "epoch 1, loss 0.7031, train acc 0.779, test acc 0.865, time 15027.3 sec\n",
      "Step 0. time since epoch: 5.034. Train acc: 0.887. Train Loss: 0.293\n",
      "Step 10. time since epoch: 52.965. Train acc: 0.858. Train Loss: 0.379\n",
      "Step 20. time since epoch: 100.827. Train acc: 0.862. Train Loss: 0.369\n",
      "Step 30. time since epoch: 148.583. Train acc: 0.861. Train Loss: 0.373\n",
      "Step 40. time since epoch: 196.523. Train acc: 0.858. Train Loss: 0.380\n",
      "Step 50. time since epoch: 244.247. Train acc: 0.859. Train Loss: 0.373\n",
      "Step 60. time since epoch: 292.108. Train acc: 0.859. Train Loss: 0.377\n",
      "Step 70. time since epoch: 339.927. Train acc: 0.860. Train Loss: 0.377\n",
      "Step 80. time since epoch: 387.613. Train acc: 0.862. Train Loss: 0.374\n",
      "Step 90. time since epoch: 435.610. Train acc: 0.862. Train Loss: 0.374\n",
      "Step 100. time since epoch: 483.365. Train acc: 0.862. Train Loss: 0.374\n",
      "Step 110. time since epoch: 531.382. Train acc: 0.862. Train Loss: 0.372\n",
      "Step 120. time since epoch: 579.653. Train acc: 0.863. Train Loss: 0.369\n",
      "Step 130. time since epoch: 627.369. Train acc: 0.863. Train Loss: 0.369\n",
      "Step 140. time since epoch: 675.219. Train acc: 0.862. Train Loss: 0.369\n",
      "Step 150. time since epoch: 723.192. Train acc: 0.862. Train Loss: 0.371\n",
      "Step 160. time since epoch: 771.614. Train acc: 0.862. Train Loss: 0.371\n",
      "Step 170. time since epoch: 820.004. Train acc: 0.863. Train Loss: 0.369\n",
      "Step 180. time since epoch: 867.966. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 190. time since epoch: 915.825. Train acc: 0.863. Train Loss: 0.369\n",
      "Step 200. time since epoch: 963.574. Train acc: 0.864. Train Loss: 0.368\n",
      "Step 210. time since epoch: 1011.507. Train acc: 0.864. Train Loss: 0.369\n",
      "Step 220. time since epoch: 1059.252. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 230. time since epoch: 1107.019. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 240. time since epoch: 1154.560. Train acc: 0.863. Train Loss: 0.371\n",
      "Step 250. time since epoch: 1202.288. Train acc: 0.863. Train Loss: 0.371\n",
      "Step 260. time since epoch: 1250.283. Train acc: 0.863. Train Loss: 0.371\n",
      "Step 270. time since epoch: 1298.080. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 280. time since epoch: 1346.023. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 290. time since epoch: 1393.849. Train acc: 0.864. Train Loss: 0.369\n",
      "Step 300. time since epoch: 1441.625. Train acc: 0.864. Train Loss: 0.369\n",
      "Step 310. time since epoch: 1489.238. Train acc: 0.864. Train Loss: 0.370\n",
      "Step 320. time since epoch: 1537.124. Train acc: 0.864. Train Loss: 0.369\n",
      "Step 330. time since epoch: 1585.084. Train acc: 0.864. Train Loss: 0.369\n",
      "Step 340. time since epoch: 1633.132. Train acc: 0.864. Train Loss: 0.369\n",
      "Step 350. time since epoch: 1681.956. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 360. time since epoch: 1729.770. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 370. time since epoch: 1777.562. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 380. time since epoch: 1825.857. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 390. time since epoch: 1873.448. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 400. time since epoch: 1921.271. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 410. time since epoch: 1968.986. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 420. time since epoch: 2016.170. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 430. time since epoch: 2064.322. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 440. time since epoch: 2115.941. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 450. time since epoch: 2165.208. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 460. time since epoch: 2223.311. Train acc: 0.863. Train Loss: 0.370\n",
      "Step 470. time since epoch: 2282.159. Train acc: 0.863. Train Loss: 0.369\n",
      "Step 480. time since epoch: 2342.439. Train acc: 0.863. Train Loss: 0.368\n",
      "Step 490. time since epoch: 2400.044. Train acc: 0.863. Train Loss: 0.368\n",
      "Step 500. time since epoch: 2454.187. Train acc: 0.863. Train Loss: 0.368\n",
      "Step 510. time since epoch: 2511.108. Train acc: 0.863. Train Loss: 0.368\n",
      "Step 520. time since epoch: 2566.936. Train acc: 0.863. Train Loss: 0.368\n",
      "Step 530. time since epoch: 2617.854. Train acc: 0.863. Train Loss: 0.368\n",
      "Step 540. time since epoch: 2674.153. Train acc: 0.863. Train Loss: 0.367\n",
      "Step 550. time since epoch: 2730.435. Train acc: 0.864. Train Loss: 0.367\n",
      "Step 560. time since epoch: 2794.164. Train acc: 0.864. Train Loss: 0.367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 570. time since epoch: 2857.746. Train acc: 0.864. Train Loss: 0.367\n",
      "Step 580. time since epoch: 2921.359. Train acc: 0.864. Train Loss: 0.367\n",
      "Step 590. time since epoch: 2985.778. Train acc: 0.864. Train Loss: 0.367\n",
      "Step 600. time since epoch: 3051.164. Train acc: 0.864. Train Loss: 0.366\n",
      "Step 610. time since epoch: 3114.815. Train acc: 0.864. Train Loss: 0.366\n",
      "Step 620. time since epoch: 3180.751. Train acc: 0.864. Train Loss: 0.366\n",
      "Step 630. time since epoch: 3244.808. Train acc: 0.864. Train Loss: 0.366\n",
      "Step 640. time since epoch: 3309.054. Train acc: 0.864. Train Loss: 0.365\n",
      "Step 650. time since epoch: 3372.712. Train acc: 0.864. Train Loss: 0.365\n",
      "Step 660. time since epoch: 3436.495. Train acc: 0.864. Train Loss: 0.365\n",
      "Step 670. time since epoch: 3500.278. Train acc: 0.864. Train Loss: 0.364\n",
      "Step 680. time since epoch: 3567.071. Train acc: 0.865. Train Loss: 0.364\n",
      "Step 690. time since epoch: 3635.150. Train acc: 0.865. Train Loss: 0.364\n",
      "Step 700. time since epoch: 3703.173. Train acc: 0.865. Train Loss: 0.364\n",
      "Step 710. time since epoch: 3769.498. Train acc: 0.865. Train Loss: 0.364\n",
      "Step 720. time since epoch: 3835.995. Train acc: 0.865. Train Loss: 0.364\n",
      "Step 730. time since epoch: 3899.834. Train acc: 0.865. Train Loss: 0.364\n",
      "Step 740. time since epoch: 3964.980. Train acc: 0.865. Train Loss: 0.364\n",
      "Step 750. time since epoch: 4029.239. Train acc: 0.865. Train Loss: 0.364\n",
      "Step 760. time since epoch: 4092.409. Train acc: 0.865. Train Loss: 0.364\n",
      "Step 770. time since epoch: 4157.828. Train acc: 0.865. Train Loss: 0.364\n",
      "Step 780. time since epoch: 4222.403. Train acc: 0.865. Train Loss: 0.364\n",
      "Step 790. time since epoch: 4286.757. Train acc: 0.865. Train Loss: 0.363\n",
      "Step 800. time since epoch: 4350.998. Train acc: 0.865. Train Loss: 0.363\n",
      "Step 810. time since epoch: 4415.440. Train acc: 0.865. Train Loss: 0.363\n",
      "Step 820. time since epoch: 4481.639. Train acc: 0.865. Train Loss: 0.363\n",
      "Step 830. time since epoch: 4547.044. Train acc: 0.865. Train Loss: 0.363\n",
      "Step 840. time since epoch: 4611.903. Train acc: 0.865. Train Loss: 0.363\n",
      "Step 850. time since epoch: 4677.969. Train acc: 0.866. Train Loss: 0.362\n",
      "Step 860. time since epoch: 4744.506. Train acc: 0.866. Train Loss: 0.363\n",
      "Step 870. time since epoch: 4810.571. Train acc: 0.866. Train Loss: 0.362\n",
      "Step 880. time since epoch: 4876.442. Train acc: 0.866. Train Loss: 0.362\n",
      "Step 890. time since epoch: 4940.634. Train acc: 0.866. Train Loss: 0.362\n",
      "Step 900. time since epoch: 5005.062. Train acc: 0.866. Train Loss: 0.362\n",
      "Step 910. time since epoch: 5069.235. Train acc: 0.866. Train Loss: 0.362\n",
      "Step 920. time since epoch: 5134.019. Train acc: 0.866. Train Loss: 0.362\n",
      "Step 930. time since epoch: 5198.107. Train acc: 0.866. Train Loss: 0.361\n",
      "Step 940. time since epoch: 5263.322. Train acc: 0.866. Train Loss: 0.361\n",
      "Step 950. time since epoch: 5329.234. Train acc: 0.866. Train Loss: 0.361\n",
      "Step 960. time since epoch: 5395.028. Train acc: 0.866. Train Loss: 0.361\n",
      "Step 970. time since epoch: 5460.873. Train acc: 0.866. Train Loss: 0.361\n",
      "Step 980. time since epoch: 5527.395. Train acc: 0.866. Train Loss: 0.361\n",
      "Step 990. time since epoch: 5593.964. Train acc: 0.866. Train Loss: 0.361\n",
      "Step 1000. time since epoch: 5660.630. Train acc: 0.866. Train Loss: 0.361\n",
      "Step 1010. time since epoch: 5724.017. Train acc: 0.866. Train Loss: 0.361\n",
      "Step 1020. time since epoch: 5787.177. Train acc: 0.866. Train Loss: 0.361\n",
      "Step 1030. time since epoch: 5850.447. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1040. time since epoch: 5916.370. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1050. time since epoch: 5983.026. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1060. time since epoch: 6049.130. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1070. time since epoch: 6113.505. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1080. time since epoch: 6179.073. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1090. time since epoch: 6245.338. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1100. time since epoch: 6309.971. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1110. time since epoch: 6367.577. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1120. time since epoch: 6421.469. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1130. time since epoch: 6473.922. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1140. time since epoch: 6522.336. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1150. time since epoch: 6573.337. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1160. time since epoch: 6622.778. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1170. time since epoch: 6671.465. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1180. time since epoch: 6719.557. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1190. time since epoch: 6768.043. Train acc: 0.866. Train Loss: 0.360\n",
      "Step 1200. time since epoch: 6816.526. Train acc: 0.866. Train Loss: 0.359\n",
      "Step 1210. time since epoch: 6865.256. Train acc: 0.866. Train Loss: 0.359\n",
      "Step 1220. time since epoch: 6915.704. Train acc: 0.866. Train Loss: 0.359\n",
      "Step 1230. time since epoch: 6964.777. Train acc: 0.866. Train Loss: 0.359\n",
      "Step 1240. time since epoch: 7014.799. Train acc: 0.867. Train Loss: 0.359\n",
      "Step 1250. time since epoch: 7064.452. Train acc: 0.867. Train Loss: 0.359\n",
      "Step 1260. time since epoch: 7114.697. Train acc: 0.867. Train Loss: 0.359\n",
      "Step 1270. time since epoch: 7163.373. Train acc: 0.867. Train Loss: 0.359\n",
      "Step 1280. time since epoch: 7211.554. Train acc: 0.867. Train Loss: 0.359\n",
      "Step 1290. time since epoch: 7261.100. Train acc: 0.867. Train Loss: 0.359\n",
      "Step 1300. time since epoch: 7310.360. Train acc: 0.867. Train Loss: 0.359\n",
      "Step 1310. time since epoch: 7360.001. Train acc: 0.867. Train Loss: 0.358\n",
      "Step 1320. time since epoch: 7409.875. Train acc: 0.867. Train Loss: 0.358\n",
      "Step 1330. time since epoch: 7459.654. Train acc: 0.867. Train Loss: 0.358\n",
      "Step 1340. time since epoch: 7509.098. Train acc: 0.867. Train Loss: 0.358\n",
      "Step 1350. time since epoch: 7558.522. Train acc: 0.867. Train Loss: 0.358\n",
      "Step 1360. time since epoch: 7608.046. Train acc: 0.867. Train Loss: 0.358\n",
      "Step 1370. time since epoch: 7657.432. Train acc: 0.867. Train Loss: 0.358\n",
      "Step 1380. time since epoch: 7707.006. Train acc: 0.867. Train Loss: 0.358\n",
      "Step 1390. time since epoch: 7756.411. Train acc: 0.867. Train Loss: 0.358\n",
      "Step 1400. time since epoch: 7806.241. Train acc: 0.867. Train Loss: 0.358\n",
      "Step 1410. time since epoch: 7856.415. Train acc: 0.867. Train Loss: 0.358\n",
      "Step 1420. time since epoch: 7906.151. Train acc: 0.867. Train Loss: 0.358\n",
      "Step 1430. time since epoch: 7955.973. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1440. time since epoch: 8006.017. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1450. time since epoch: 8056.292. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1460. time since epoch: 8105.499. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1470. time since epoch: 8155.089. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1480. time since epoch: 8204.342. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1490. time since epoch: 8254.253. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1500. time since epoch: 8303.618. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1510. time since epoch: 8352.788. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1520. time since epoch: 8402.523. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1530. time since epoch: 8452.472. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1540. time since epoch: 8501.830. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1550. time since epoch: 8551.490. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1560. time since epoch: 8601.147. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1570. time since epoch: 8650.197. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1580. time since epoch: 8699.922. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1590. time since epoch: 8749.015. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1600. time since epoch: 8796.914. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1610. time since epoch: 8846.303. Train acc: 0.867. Train Loss: 0.357\n",
      "Step 1620. time since epoch: 8895.402. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1630. time since epoch: 8945.353. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1640. time since epoch: 8994.429. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1650. time since epoch: 9044.290. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1660. time since epoch: 9092.829. Train acc: 0.867. Train Loss: 0.356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1670. time since epoch: 9141.089. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1680. time since epoch: 9189.388. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1690. time since epoch: 9239.018. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1700. time since epoch: 9288.684. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1710. time since epoch: 9338.223. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1720. time since epoch: 9387.640. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1730. time since epoch: 9436.975. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1740. time since epoch: 9485.626. Train acc: 0.868. Train Loss: 0.356\n",
      "Step 1750. time since epoch: 9534.438. Train acc: 0.868. Train Loss: 0.356\n",
      "Step 1760. time since epoch: 9583.179. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1770. time since epoch: 9631.129. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1780. time since epoch: 9680.174. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1790. time since epoch: 9729.172. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1800. time since epoch: 9778.885. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1810. time since epoch: 9828.466. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1820. time since epoch: 9876.805. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1830. time since epoch: 9924.415. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1840. time since epoch: 9972.617. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1850. time since epoch: 10021.050. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1860. time since epoch: 10069.932. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1870. time since epoch: 10118.343. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1880. time since epoch: 10167.369. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1890. time since epoch: 10216.282. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1900. time since epoch: 10265.613. Train acc: 0.867. Train Loss: 0.356\n",
      "Step 1910. time since epoch: 10314.740. Train acc: 0.867. Train Loss: 0.355\n",
      "Step 1920. time since epoch: 10363.680. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 1930. time since epoch: 10413.054. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 1940. time since epoch: 10462.652. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 1950. time since epoch: 10511.909. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 1960. time since epoch: 10559.956. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 1970. time since epoch: 10608.075. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 1980. time since epoch: 10656.977. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 1990. time since epoch: 10706.332. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 2000. time since epoch: 10756.078. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 2010. time since epoch: 10805.163. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 2020. time since epoch: 10854.629. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 2030. time since epoch: 10902.711. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 2040. time since epoch: 10950.929. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 2050. time since epoch: 10998.728. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 2060. time since epoch: 11046.656. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 2070. time since epoch: 11094.299. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 2080. time since epoch: 11142.020. Train acc: 0.868. Train Loss: 0.355\n",
      "Step 2090. time since epoch: 11190.922. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2100. time since epoch: 11239.917. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2110. time since epoch: 11289.107. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2120. time since epoch: 11337.942. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2130. time since epoch: 11387.297. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2140. time since epoch: 11436.397. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2150. time since epoch: 11486.071. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2160. time since epoch: 11535.522. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2170. time since epoch: 11584.658. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2180. time since epoch: 11633.723. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2190. time since epoch: 11683.159. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2200. time since epoch: 11731.563. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2210. time since epoch: 11779.352. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2220. time since epoch: 11827.205. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2230. time since epoch: 11875.525. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2240. time since epoch: 11924.427. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2250. time since epoch: 11973.217. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2260. time since epoch: 12022.455. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2270. time since epoch: 12071.906. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2280. time since epoch: 12120.875. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2290. time since epoch: 12169.867. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2300. time since epoch: 12218.430. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2310. time since epoch: 12267.762. Train acc: 0.868. Train Loss: 0.354\n",
      "Step 2320. time since epoch: 12317.188. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2330. time since epoch: 12366.379. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2340. time since epoch: 12415.627. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2350. time since epoch: 12465.267. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2360. time since epoch: 12514.600. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2370. time since epoch: 12564.527. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2380. time since epoch: 12613.537. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2390. time since epoch: 12662.262. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2400. time since epoch: 12710.567. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2410. time since epoch: 12758.583. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2420. time since epoch: 12807.348. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2430. time since epoch: 12856.366. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2440. time since epoch: 12908.020. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2450. time since epoch: 12968.666. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2460. time since epoch: 13027.018. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2470. time since epoch: 13084.575. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2480. time since epoch: 13144.750. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2490. time since epoch: 13202.902. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2500. time since epoch: 13264.383. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2510. time since epoch: 13323.295. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2520. time since epoch: 13381.469. Train acc: 0.868. Train Loss: 0.353\n",
      "Step 2530. time since epoch: 13439.502. Train acc: 0.868. Train Loss: 0.352\n",
      "Step 2540. time since epoch: 13500.773. Train acc: 0.868. Train Loss: 0.352\n",
      "Step 2550. time since epoch: 13554.351. Train acc: 0.868. Train Loss: 0.352\n",
      "Step 2560. time since epoch: 13605.427. Train acc: 0.868. Train Loss: 0.352\n",
      "Step 2570. time since epoch: 13657.805. Train acc: 0.868. Train Loss: 0.352\n",
      "Step 2580. time since epoch: 13719.893. Train acc: 0.868. Train Loss: 0.352\n",
      "Step 2590. time since epoch: 13781.243. Train acc: 0.868. Train Loss: 0.352\n",
      "Step 2600. time since epoch: 13831.336. Train acc: 0.868. Train Loss: 0.352\n",
      "Step 2610. time since epoch: 13880.846. Train acc: 0.868. Train Loss: 0.352\n",
      "Step 2620. time since epoch: 13943.047. Train acc: 0.868. Train Loss: 0.352\n",
      "Step 2630. time since epoch: 13992.717. Train acc: 0.868. Train Loss: 0.352\n",
      "Step 2640. time since epoch: 14041.355. Train acc: 0.868. Train Loss: 0.352\n",
      "Step 2650. time since epoch: 14089.442. Train acc: 0.869. Train Loss: 0.352\n",
      "Step 2660. time since epoch: 14155.706. Train acc: 0.869. Train Loss: 0.352\n",
      "Step 2670. time since epoch: 14214.631. Train acc: 0.869. Train Loss: 0.352\n",
      "Step 2680. time since epoch: 14273.020. Train acc: 0.869. Train Loss: 0.352\n",
      "Step 2690. time since epoch: 14338.114. Train acc: 0.869. Train Loss: 0.352\n",
      "Step 2700. time since epoch: 14390.239. Train acc: 0.869. Train Loss: 0.352\n",
      "Step 2710. time since epoch: 14445.227. Train acc: 0.869. Train Loss: 0.352\n",
      "Step 2720. time since epoch: 14495.799. Train acc: 0.869. Train Loss: 0.352\n",
      "--------------------\n",
      "epoch 2, loss 0.3516, train acc 0.869, test acc 0.868, time 15570.7 sec\n",
      "Step 0. time since epoch: 5.439. Train acc: 0.898. Train Loss: 0.268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10. time since epoch: 60.036. Train acc: 0.870. Train Loss: 0.348\n",
      "Step 20. time since epoch: 117.258. Train acc: 0.871. Train Loss: 0.341\n",
      "Step 30. time since epoch: 170.679. Train acc: 0.871. Train Loss: 0.339\n",
      "Step 40. time since epoch: 226.426. Train acc: 0.867. Train Loss: 0.349\n",
      "Step 50. time since epoch: 279.857. Train acc: 0.868. Train Loss: 0.344\n",
      "Step 60. time since epoch: 336.299. Train acc: 0.867. Train Loss: 0.348\n",
      "Step 70. time since epoch: 389.447. Train acc: 0.868. Train Loss: 0.346\n",
      "Step 80. time since epoch: 443.873. Train acc: 0.870. Train Loss: 0.344\n",
      "Step 90. time since epoch: 499.471. Train acc: 0.870. Train Loss: 0.344\n",
      "Step 100. time since epoch: 554.563. Train acc: 0.871. Train Loss: 0.342\n",
      "Step 110. time since epoch: 608.114. Train acc: 0.871. Train Loss: 0.341\n",
      "Step 120. time since epoch: 660.701. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 130. time since epoch: 712.980. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 140. time since epoch: 765.589. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 150. time since epoch: 819.173. Train acc: 0.871. Train Loss: 0.341\n",
      "Step 160. time since epoch: 871.062. Train acc: 0.871. Train Loss: 0.340\n",
      "Step 170. time since epoch: 923.196. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 180. time since epoch: 975.581. Train acc: 0.872. Train Loss: 0.338\n",
      "Step 190. time since epoch: 1028.312. Train acc: 0.873. Train Loss: 0.338\n",
      "Step 200. time since epoch: 1082.887. Train acc: 0.873. Train Loss: 0.337\n",
      "Step 210. time since epoch: 1137.380. Train acc: 0.873. Train Loss: 0.337\n",
      "Step 220. time since epoch: 1188.498. Train acc: 0.872. Train Loss: 0.338\n",
      "Step 230. time since epoch: 1238.489. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 240. time since epoch: 1287.959. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 250. time since epoch: 1337.791. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 260. time since epoch: 1386.936. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 270. time since epoch: 1436.424. Train acc: 0.873. Train Loss: 0.338\n",
      "Step 280. time since epoch: 1486.237. Train acc: 0.873. Train Loss: 0.338\n",
      "Step 290. time since epoch: 1534.953. Train acc: 0.873. Train Loss: 0.338\n",
      "Step 300. time since epoch: 1583.442. Train acc: 0.873. Train Loss: 0.338\n",
      "Step 310. time since epoch: 1631.979. Train acc: 0.873. Train Loss: 0.338\n",
      "Step 320. time since epoch: 1679.962. Train acc: 0.873. Train Loss: 0.338\n",
      "Step 330. time since epoch: 1728.729. Train acc: 0.873. Train Loss: 0.337\n",
      "Step 340. time since epoch: 1777.627. Train acc: 0.872. Train Loss: 0.338\n",
      "Step 350. time since epoch: 1826.992. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 360. time since epoch: 1876.956. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 370. time since epoch: 1926.281. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 380. time since epoch: 1975.780. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 390. time since epoch: 2025.302. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 400. time since epoch: 2074.531. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 410. time since epoch: 2123.963. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 420. time since epoch: 2174.748. Train acc: 0.871. Train Loss: 0.339\n",
      "Step 430. time since epoch: 2224.485. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 440. time since epoch: 2274.024. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 450. time since epoch: 2323.331. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 460. time since epoch: 2372.851. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 470. time since epoch: 2422.797. Train acc: 0.872. Train Loss: 0.339\n",
      "Step 480. time since epoch: 2471.921. Train acc: 0.872. Train Loss: 0.338\n",
      "Step 490. time since epoch: 2521.397. Train acc: 0.872. Train Loss: 0.338\n",
      "Step 500. time since epoch: 2570.941. Train acc: 0.872. Train Loss: 0.338\n",
      "Step 510. time since epoch: 2621.826. Train acc: 0.872. Train Loss: 0.338\n",
      "Step 520. time since epoch: 2671.441. Train acc: 0.872. Train Loss: 0.338\n",
      "Step 530. time since epoch: 2719.883. Train acc: 0.872. Train Loss: 0.338\n",
      "Step 540. time since epoch: 2768.411. Train acc: 0.872. Train Loss: 0.337\n",
      "Step 550. time since epoch: 2817.946. Train acc: 0.873. Train Loss: 0.337\n",
      "Step 560. time since epoch: 2866.777. Train acc: 0.873. Train Loss: 0.337\n",
      "Step 570. time since epoch: 2915.365. Train acc: 0.873. Train Loss: 0.337\n",
      "Step 580. time since epoch: 2964.718. Train acc: 0.873. Train Loss: 0.337\n",
      "Step 590. time since epoch: 3014.407. Train acc: 0.873. Train Loss: 0.337\n",
      "Step 600. time since epoch: 3064.344. Train acc: 0.873. Train Loss: 0.336\n",
      "Step 610. time since epoch: 3114.215. Train acc: 0.873. Train Loss: 0.336\n",
      "Step 620. time since epoch: 3163.716. Train acc: 0.873. Train Loss: 0.336\n",
      "Step 630. time since epoch: 3212.280. Train acc: 0.873. Train Loss: 0.336\n",
      "Step 640. time since epoch: 3261.406. Train acc: 0.873. Train Loss: 0.336\n",
      "Step 650. time since epoch: 3310.852. Train acc: 0.873. Train Loss: 0.335\n",
      "Step 660. time since epoch: 3359.316. Train acc: 0.873. Train Loss: 0.335\n",
      "Step 670. time since epoch: 3408.320. Train acc: 0.873. Train Loss: 0.335\n",
      "Step 680. time since epoch: 3458.247. Train acc: 0.873. Train Loss: 0.335\n",
      "Step 690. time since epoch: 3507.085. Train acc: 0.873. Train Loss: 0.335\n",
      "Step 700. time since epoch: 3556.219. Train acc: 0.874. Train Loss: 0.335\n",
      "Step 710. time since epoch: 3605.841. Train acc: 0.874. Train Loss: 0.335\n",
      "Step 720. time since epoch: 3654.170. Train acc: 0.873. Train Loss: 0.335\n",
      "Step 730. time since epoch: 3703.334. Train acc: 0.873. Train Loss: 0.335\n",
      "Step 740. time since epoch: 3755.376. Train acc: 0.873. Train Loss: 0.335\n",
      "Step 750. time since epoch: 3807.336. Train acc: 0.874. Train Loss: 0.335\n",
      "Step 760. time since epoch: 3859.699. Train acc: 0.873. Train Loss: 0.335\n",
      "Step 770. time since epoch: 3912.930. Train acc: 0.874. Train Loss: 0.334\n",
      "Step 780. time since epoch: 3965.065. Train acc: 0.874. Train Loss: 0.334\n",
      "Step 790. time since epoch: 4016.967. Train acc: 0.874. Train Loss: 0.334\n",
      "Step 800. time since epoch: 4068.764. Train acc: 0.874. Train Loss: 0.334\n",
      "Step 810. time since epoch: 4120.556. Train acc: 0.874. Train Loss: 0.334\n",
      "Step 820. time since epoch: 4173.578. Train acc: 0.874. Train Loss: 0.334\n",
      "Step 830. time since epoch: 4227.032. Train acc: 0.874. Train Loss: 0.334\n",
      "Step 840. time since epoch: 4279.754. Train acc: 0.874. Train Loss: 0.334\n",
      "Step 850. time since epoch: 4332.458. Train acc: 0.874. Train Loss: 0.333\n",
      "Step 860. time since epoch: 4385.192. Train acc: 0.874. Train Loss: 0.333\n",
      "Step 870. time since epoch: 4437.479. Train acc: 0.874. Train Loss: 0.333\n",
      "Step 880. time since epoch: 4490.999. Train acc: 0.874. Train Loss: 0.333\n",
      "Step 890. time since epoch: 4543.684. Train acc: 0.874. Train Loss: 0.333\n",
      "Step 900. time since epoch: 4596.235. Train acc: 0.874. Train Loss: 0.333\n",
      "Step 910. time since epoch: 4648.633. Train acc: 0.874. Train Loss: 0.332\n",
      "Step 920. time since epoch: 4702.573. Train acc: 0.874. Train Loss: 0.332\n",
      "Step 930. time since epoch: 4755.841. Train acc: 0.874. Train Loss: 0.332\n",
      "Step 940. time since epoch: 4808.685. Train acc: 0.874. Train Loss: 0.332\n",
      "Step 950. time since epoch: 4861.405. Train acc: 0.874. Train Loss: 0.332\n",
      "Step 960. time since epoch: 4913.952. Train acc: 0.874. Train Loss: 0.332\n",
      "Step 970. time since epoch: 4966.730. Train acc: 0.874. Train Loss: 0.332\n",
      "Step 980. time since epoch: 5020.544. Train acc: 0.874. Train Loss: 0.332\n",
      "Step 990. time since epoch: 5072.680. Train acc: 0.874. Train Loss: 0.332\n",
      "Step 1000. time since epoch: 5125.581. Train acc: 0.874. Train Loss: 0.332\n",
      "Step 1010. time since epoch: 5177.957. Train acc: 0.874. Train Loss: 0.332\n",
      "Step 1020. time since epoch: 5231.250. Train acc: 0.874. Train Loss: 0.332\n",
      "Step 1030. time since epoch: 5283.759. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1040. time since epoch: 5336.340. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1050. time since epoch: 5390.268. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1060. time since epoch: 5442.329. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1070. time since epoch: 5494.682. Train acc: 0.874. Train Loss: 0.332\n",
      "Step 1080. time since epoch: 5547.518. Train acc: 0.874. Train Loss: 0.332\n",
      "Step 1090. time since epoch: 5600.546. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1100. time since epoch: 5652.962. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1110. time since epoch: 5705.549. Train acc: 0.874. Train Loss: 0.331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1120. time since epoch: 5760.408. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1130. time since epoch: 5812.992. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1140. time since epoch: 5863.970. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1150. time since epoch: 5912.742. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1160. time since epoch: 5961.936. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1170. time since epoch: 6012.195. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1180. time since epoch: 6062.044. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1190. time since epoch: 6114.440. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1200. time since epoch: 6167.145. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1210. time since epoch: 6222.362. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1220. time since epoch: 6279.046. Train acc: 0.874. Train Loss: 0.331\n",
      "Step 1230. time since epoch: 6331.325. Train acc: 0.875. Train Loss: 0.330\n",
      "Step 1240. time since epoch: 6386.189. Train acc: 0.875. Train Loss: 0.330\n",
      "Step 1250. time since epoch: 6441.399. Train acc: 0.875. Train Loss: 0.330\n",
      "Step 1260. time since epoch: 6498.329. Train acc: 0.875. Train Loss: 0.330\n",
      "Step 1270. time since epoch: 6552.824. Train acc: 0.875. Train Loss: 0.330\n",
      "Step 1280. time since epoch: 6607.797. Train acc: 0.875. Train Loss: 0.330\n",
      "Step 1290. time since epoch: 6661.815. Train acc: 0.875. Train Loss: 0.330\n",
      "Step 1300. time since epoch: 6715.124. Train acc: 0.875. Train Loss: 0.330\n",
      "Step 1310. time since epoch: 6768.603. Train acc: 0.875. Train Loss: 0.330\n",
      "Step 1320. time since epoch: 6821.828. Train acc: 0.875. Train Loss: 0.330\n",
      "Step 1330. time since epoch: 6875.439. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1340. time since epoch: 6929.284. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1350. time since epoch: 6984.959. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1360. time since epoch: 7041.969. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1370. time since epoch: 7097.944. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1380. time since epoch: 7152.050. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1390. time since epoch: 7206.611. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1400. time since epoch: 7260.536. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1410. time since epoch: 7313.946. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1420. time since epoch: 7368.275. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1430. time since epoch: 7417.682. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1440. time since epoch: 7465.812. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1450. time since epoch: 7513.642. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1460. time since epoch: 7561.310. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1470. time since epoch: 7609.353. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1480. time since epoch: 7657.313. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1490. time since epoch: 7706.997. Train acc: 0.875. Train Loss: 0.329\n",
      "Step 1500. time since epoch: 7755.026. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1510. time since epoch: 7803.044. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1520. time since epoch: 7850.938. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1530. time since epoch: 7898.847. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1540. time since epoch: 7946.820. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1550. time since epoch: 7994.572. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1560. time since epoch: 8042.431. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1570. time since epoch: 8090.359. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1580. time since epoch: 8138.326. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1590. time since epoch: 8186.369. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1600. time since epoch: 8234.391. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1610. time since epoch: 8282.546. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1620. time since epoch: 8330.390. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1630. time since epoch: 8378.017. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1640. time since epoch: 8426.172. Train acc: 0.876. Train Loss: 0.328\n",
      "Step 1650. time since epoch: 8474.054. Train acc: 0.876. Train Loss: 0.328\n",
      "Step 1660. time since epoch: 8521.924. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1670. time since epoch: 8569.537. Train acc: 0.875. Train Loss: 0.327\n",
      "Step 1680. time since epoch: 8618.036. Train acc: 0.875. Train Loss: 0.327\n",
      "Step 1690. time since epoch: 8665.910. Train acc: 0.875. Train Loss: 0.327\n",
      "Step 1700. time since epoch: 8713.812. Train acc: 0.875. Train Loss: 0.327\n",
      "Step 1710. time since epoch: 8761.868. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1720. time since epoch: 8809.758. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1730. time since epoch: 8857.377. Train acc: 0.875. Train Loss: 0.327\n",
      "Step 1740. time since epoch: 8905.221. Train acc: 0.876. Train Loss: 0.327\n",
      "Step 1750. time since epoch: 8953.154. Train acc: 0.876. Train Loss: 0.327\n",
      "Step 1760. time since epoch: 9001.075. Train acc: 0.876. Train Loss: 0.327\n",
      "Step 1770. time since epoch: 9049.134. Train acc: 0.875. Train Loss: 0.327\n",
      "Step 1780. time since epoch: 9096.836. Train acc: 0.875. Train Loss: 0.327\n",
      "Step 1790. time since epoch: 9144.728. Train acc: 0.875. Train Loss: 0.327\n",
      "Step 1800. time since epoch: 9192.442. Train acc: 0.875. Train Loss: 0.327\n",
      "Step 1810. time since epoch: 9240.136. Train acc: 0.875. Train Loss: 0.328\n",
      "Step 1820. time since epoch: 9287.749. Train acc: 0.875. Train Loss: 0.327\n",
      "Step 1830. time since epoch: 9335.424. Train acc: 0.875. Train Loss: 0.327\n",
      "Step 1840. time since epoch: 9383.011. Train acc: 0.875. Train Loss: 0.327\n",
      "Step 1850. time since epoch: 9430.776. Train acc: 0.875. Train Loss: 0.327\n",
      "Step 1860. time since epoch: 9478.672. Train acc: 0.875. Train Loss: 0.327\n",
      "Step 1870. time since epoch: 9526.745. Train acc: 0.875. Train Loss: 0.327\n",
      "Step 1880. time since epoch: 9574.970. Train acc: 0.875. Train Loss: 0.327\n",
      "Step 1890. time since epoch: 9622.951. Train acc: 0.876. Train Loss: 0.327\n",
      "Step 1900. time since epoch: 9670.707. Train acc: 0.876. Train Loss: 0.327\n",
      "Step 1910. time since epoch: 9718.488. Train acc: 0.876. Train Loss: 0.327\n",
      "Step 1920. time since epoch: 9766.337. Train acc: 0.876. Train Loss: 0.327\n",
      "Step 1930. time since epoch: 9814.089. Train acc: 0.876. Train Loss: 0.327\n",
      "Step 1940. time since epoch: 9862.547. Train acc: 0.876. Train Loss: 0.327\n",
      "Step 1950. time since epoch: 9910.445. Train acc: 0.876. Train Loss: 0.327\n",
      "Step 1960. time since epoch: 9958.472. Train acc: 0.876. Train Loss: 0.327\n",
      "Step 1970. time since epoch: 10006.321. Train acc: 0.876. Train Loss: 0.327\n",
      "Step 1980. time since epoch: 10054.091. Train acc: 0.876. Train Loss: 0.327\n",
      "Step 1990. time since epoch: 10102.037. Train acc: 0.876. Train Loss: 0.327\n",
      "Step 2000. time since epoch: 10150.147. Train acc: 0.876. Train Loss: 0.327\n",
      "Step 2010. time since epoch: 10198.247. Train acc: 0.876. Train Loss: 0.326\n",
      "Step 2020. time since epoch: 10246.182. Train acc: 0.876. Train Loss: 0.326\n",
      "Step 2030. time since epoch: 10294.253. Train acc: 0.876. Train Loss: 0.326\n",
      "Step 2040. time since epoch: 10342.210. Train acc: 0.876. Train Loss: 0.326\n",
      "Step 2050. time since epoch: 10390.202. Train acc: 0.876. Train Loss: 0.326\n",
      "Step 2060. time since epoch: 10438.466. Train acc: 0.876. Train Loss: 0.326\n",
      "Step 2070. time since epoch: 10486.308. Train acc: 0.876. Train Loss: 0.326\n",
      "Step 2080. time since epoch: 10534.071. Train acc: 0.876. Train Loss: 0.326\n",
      "Step 2090. time since epoch: 10581.937. Train acc: 0.876. Train Loss: 0.326\n",
      "Step 2100. time since epoch: 10629.617. Train acc: 0.876. Train Loss: 0.326\n",
      "Step 2110. time since epoch: 10677.376. Train acc: 0.876. Train Loss: 0.326\n",
      "Step 2120. time since epoch: 10724.952. Train acc: 0.876. Train Loss: 0.326\n",
      "Step 2130. time since epoch: 10772.957. Train acc: 0.876. Train Loss: 0.326\n",
      "Step 2140. time since epoch: 10820.550. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2150. time since epoch: 10868.131. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2160. time since epoch: 10915.848. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2170. time since epoch: 10963.424. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2180. time since epoch: 11010.978. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2190. time since epoch: 11058.548. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2200. time since epoch: 11106.431. Train acc: 0.876. Train Loss: 0.325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2210. time since epoch: 11154.355. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2220. time since epoch: 11202.122. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2230. time since epoch: 11251.583. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2240. time since epoch: 11301.512. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2250. time since epoch: 11349.395. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2260. time since epoch: 11397.382. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2270. time since epoch: 11445.355. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2280. time since epoch: 11493.527. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2290. time since epoch: 11541.420. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2300. time since epoch: 11589.474. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2310. time since epoch: 11637.743. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2320. time since epoch: 11685.653. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2330. time since epoch: 11733.624. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2340. time since epoch: 11781.624. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2350. time since epoch: 11829.426. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2360. time since epoch: 11877.614. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2370. time since epoch: 11925.716. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2380. time since epoch: 11973.800. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2390. time since epoch: 12022.243. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2400. time since epoch: 12070.755. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2410. time since epoch: 12119.281. Train acc: 0.876. Train Loss: 0.325\n",
      "Step 2420. time since epoch: 12167.704. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2430. time since epoch: 12216.646. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2440. time since epoch: 12265.059. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2450. time since epoch: 12313.045. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2460. time since epoch: 12361.455. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2470. time since epoch: 12409.607. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2480. time since epoch: 12458.198. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2490. time since epoch: 12507.022. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2500. time since epoch: 12556.569. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2510. time since epoch: 12605.111. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2520. time since epoch: 12653.602. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2530. time since epoch: 12702.057. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2540. time since epoch: 12750.730. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2550. time since epoch: 12799.986. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2560. time since epoch: 12848.641. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2570. time since epoch: 12897.219. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2580. time since epoch: 12945.355. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2590. time since epoch: 12993.683. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2600. time since epoch: 13042.190. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2610. time since epoch: 13090.665. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2620. time since epoch: 13139.049. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2630. time since epoch: 13187.006. Train acc: 0.876. Train Loss: 0.324\n",
      "Step 2640. time since epoch: 13234.843. Train acc: 0.876. Train Loss: 0.323\n",
      "Step 2650. time since epoch: 13282.907. Train acc: 0.877. Train Loss: 0.323\n",
      "Step 2660. time since epoch: 13330.699. Train acc: 0.877. Train Loss: 0.323\n",
      "Step 2670. time since epoch: 13378.832. Train acc: 0.877. Train Loss: 0.323\n",
      "Step 2680. time since epoch: 13426.622. Train acc: 0.877. Train Loss: 0.323\n",
      "Step 2690. time since epoch: 13474.321. Train acc: 0.877. Train Loss: 0.323\n",
      "Step 2700. time since epoch: 13521.921. Train acc: 0.877. Train Loss: 0.323\n",
      "Step 2710. time since epoch: 13570.761. Train acc: 0.877. Train Loss: 0.323\n",
      "Step 2720. time since epoch: 13619.506. Train acc: 0.877. Train Loss: 0.323\n",
      "--------------------\n",
      "epoch 3, loss 0.3230, train acc 0.877, test acc 0.868, time 14560.0 sec\n",
      "Step 0. time since epoch: 4.990. Train acc: 0.895. Train Loss: 0.255\n",
      "Step 10. time since epoch: 54.396. Train acc: 0.874. Train Loss: 0.321\n",
      "Step 20. time since epoch: 103.287. Train acc: 0.876. Train Loss: 0.315\n",
      "Step 30. time since epoch: 151.692. Train acc: 0.878. Train Loss: 0.313\n",
      "Step 40. time since epoch: 200.520. Train acc: 0.874. Train Loss: 0.320\n",
      "Step 50. time since epoch: 250.387. Train acc: 0.876. Train Loss: 0.315\n",
      "Step 60. time since epoch: 300.149. Train acc: 0.875. Train Loss: 0.320\n",
      "Step 70. time since epoch: 349.160. Train acc: 0.876. Train Loss: 0.318\n",
      "Step 80. time since epoch: 398.166. Train acc: 0.878. Train Loss: 0.316\n",
      "Step 90. time since epoch: 446.996. Train acc: 0.878. Train Loss: 0.316\n",
      "Step 100. time since epoch: 495.588. Train acc: 0.880. Train Loss: 0.314\n",
      "Step 110. time since epoch: 544.053. Train acc: 0.881. Train Loss: 0.312\n",
      "Step 120. time since epoch: 592.626. Train acc: 0.881. Train Loss: 0.310\n",
      "Step 130. time since epoch: 641.524. Train acc: 0.881. Train Loss: 0.310\n",
      "Step 140. time since epoch: 690.237. Train acc: 0.881. Train Loss: 0.309\n",
      "Step 150. time since epoch: 738.856. Train acc: 0.880. Train Loss: 0.311\n",
      "Step 160. time since epoch: 787.139. Train acc: 0.881. Train Loss: 0.310\n",
      "Step 170. time since epoch: 835.734. Train acc: 0.881. Train Loss: 0.309\n",
      "Step 180. time since epoch: 884.504. Train acc: 0.881. Train Loss: 0.308\n",
      "Step 190. time since epoch: 933.319. Train acc: 0.882. Train Loss: 0.308\n",
      "Step 200. time since epoch: 982.064. Train acc: 0.882. Train Loss: 0.307\n",
      "Step 210. time since epoch: 1031.058. Train acc: 0.882. Train Loss: 0.308\n",
      "Step 220. time since epoch: 1079.672. Train acc: 0.882. Train Loss: 0.308\n",
      "Step 230. time since epoch: 1128.093. Train acc: 0.881. Train Loss: 0.309\n",
      "Step 240. time since epoch: 1176.604. Train acc: 0.881. Train Loss: 0.309\n",
      "Step 250. time since epoch: 1225.186. Train acc: 0.882. Train Loss: 0.309\n",
      "Step 260. time since epoch: 1275.004. Train acc: 0.882. Train Loss: 0.309\n",
      "Step 270. time since epoch: 1326.561. Train acc: 0.882. Train Loss: 0.308\n",
      "Step 280. time since epoch: 1379.337. Train acc: 0.882. Train Loss: 0.308\n",
      "Step 290. time since epoch: 1432.304. Train acc: 0.882. Train Loss: 0.308\n",
      "Step 300. time since epoch: 1485.510. Train acc: 0.882. Train Loss: 0.308\n",
      "Step 310. time since epoch: 1537.580. Train acc: 0.882. Train Loss: 0.308\n",
      "Step 320. time since epoch: 1586.741. Train acc: 0.882. Train Loss: 0.308\n",
      "Step 330. time since epoch: 1634.834. Train acc: 0.882. Train Loss: 0.308\n",
      "Step 340. time since epoch: 1682.646. Train acc: 0.882. Train Loss: 0.309\n",
      "Step 350. time since epoch: 1730.593. Train acc: 0.882. Train Loss: 0.310\n",
      "Step 360. time since epoch: 1778.365. Train acc: 0.882. Train Loss: 0.310\n",
      "Step 370. time since epoch: 1826.358. Train acc: 0.882. Train Loss: 0.310\n",
      "Step 380. time since epoch: 1874.259. Train acc: 0.882. Train Loss: 0.310\n",
      "Step 390. time since epoch: 1922.077. Train acc: 0.881. Train Loss: 0.310\n",
      "Step 400. time since epoch: 1970.058. Train acc: 0.881. Train Loss: 0.310\n",
      "Step 410. time since epoch: 2018.294. Train acc: 0.881. Train Loss: 0.310\n",
      "Step 420. time since epoch: 2066.324. Train acc: 0.881. Train Loss: 0.310\n",
      "Step 430. time since epoch: 2114.172. Train acc: 0.881. Train Loss: 0.311\n",
      "Step 440. time since epoch: 2162.192. Train acc: 0.881. Train Loss: 0.311\n",
      "Step 450. time since epoch: 2210.748. Train acc: 0.881. Train Loss: 0.311\n",
      "Step 460. time since epoch: 2258.739. Train acc: 0.881. Train Loss: 0.311\n",
      "Step 470. time since epoch: 2307.005. Train acc: 0.881. Train Loss: 0.311\n",
      "Step 480. time since epoch: 2355.270. Train acc: 0.881. Train Loss: 0.310\n",
      "Step 490. time since epoch: 2403.294. Train acc: 0.881. Train Loss: 0.310\n",
      "Step 500. time since epoch: 2451.487. Train acc: 0.881. Train Loss: 0.310\n",
      "Step 510. time since epoch: 2499.324. Train acc: 0.881. Train Loss: 0.309\n",
      "Step 520. time since epoch: 2547.456. Train acc: 0.881. Train Loss: 0.310\n",
      "Step 530. time since epoch: 2595.447. Train acc: 0.881. Train Loss: 0.309\n",
      "Step 540. time since epoch: 2643.710. Train acc: 0.881. Train Loss: 0.309\n",
      "Step 550. time since epoch: 2691.802. Train acc: 0.881. Train Loss: 0.309\n",
      "Step 560. time since epoch: 2739.767. Train acc: 0.881. Train Loss: 0.309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 570. time since epoch: 2787.836. Train acc: 0.881. Train Loss: 0.309\n",
      "Step 580. time since epoch: 2835.915. Train acc: 0.881. Train Loss: 0.309\n",
      "Step 590. time since epoch: 2884.103. Train acc: 0.881. Train Loss: 0.309\n",
      "Step 600. time since epoch: 2932.340. Train acc: 0.881. Train Loss: 0.309\n",
      "Step 610. time since epoch: 2980.201. Train acc: 0.881. Train Loss: 0.308\n",
      "Step 620. time since epoch: 3028.198. Train acc: 0.882. Train Loss: 0.308\n",
      "Step 630. time since epoch: 3076.616. Train acc: 0.882. Train Loss: 0.308\n",
      "Step 640. time since epoch: 3124.657. Train acc: 0.882. Train Loss: 0.308\n",
      "Step 650. time since epoch: 3172.818. Train acc: 0.882. Train Loss: 0.307\n",
      "Step 660. time since epoch: 3220.838. Train acc: 0.882. Train Loss: 0.307\n",
      "Step 670. time since epoch: 3268.662. Train acc: 0.882. Train Loss: 0.307\n",
      "Step 680. time since epoch: 3317.009. Train acc: 0.882. Train Loss: 0.307\n",
      "Step 690. time since epoch: 3364.837. Train acc: 0.882. Train Loss: 0.307\n",
      "Step 700. time since epoch: 3412.818. Train acc: 0.882. Train Loss: 0.307\n",
      "Step 710. time since epoch: 3460.959. Train acc: 0.882. Train Loss: 0.307\n",
      "Step 720. time since epoch: 3509.260. Train acc: 0.882. Train Loss: 0.307\n",
      "Step 730. time since epoch: 3557.575. Train acc: 0.882. Train Loss: 0.307\n",
      "Step 740. time since epoch: 3605.631. Train acc: 0.882. Train Loss: 0.307\n",
      "Step 750. time since epoch: 3653.396. Train acc: 0.882. Train Loss: 0.307\n",
      "Step 760. time since epoch: 3701.378. Train acc: 0.882. Train Loss: 0.307\n",
      "Step 770. time since epoch: 3749.294. Train acc: 0.882. Train Loss: 0.307\n",
      "Step 780. time since epoch: 3797.403. Train acc: 0.882. Train Loss: 0.306\n",
      "Step 790. time since epoch: 3845.463. Train acc: 0.882. Train Loss: 0.307\n",
      "Step 800. time since epoch: 3893.497. Train acc: 0.882. Train Loss: 0.306\n",
      "Step 810. time since epoch: 3941.592. Train acc: 0.882. Train Loss: 0.307\n",
      "Step 820. time since epoch: 3989.817. Train acc: 0.882. Train Loss: 0.306\n",
      "Step 830. time since epoch: 4038.212. Train acc: 0.882. Train Loss: 0.306\n",
      "Step 840. time since epoch: 4086.438. Train acc: 0.882. Train Loss: 0.306\n",
      "Step 850. time since epoch: 4134.567. Train acc: 0.882. Train Loss: 0.306\n",
      "Step 860. time since epoch: 4182.600. Train acc: 0.882. Train Loss: 0.306\n",
      "Step 870. time since epoch: 4230.614. Train acc: 0.882. Train Loss: 0.305\n",
      "Step 880. time since epoch: 4278.884. Train acc: 0.882. Train Loss: 0.305\n",
      "Step 890. time since epoch: 4326.853. Train acc: 0.882. Train Loss: 0.305\n",
      "Step 900. time since epoch: 4374.870. Train acc: 0.882. Train Loss: 0.305\n",
      "Step 910. time since epoch: 4423.100. Train acc: 0.883. Train Loss: 0.305\n",
      "Step 920. time since epoch: 4471.180. Train acc: 0.883. Train Loss: 0.305\n",
      "Step 930. time since epoch: 4519.103. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 940. time since epoch: 4567.187. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 950. time since epoch: 4615.243. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 960. time since epoch: 4663.430. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 970. time since epoch: 4711.304. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 980. time since epoch: 4759.429. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 990. time since epoch: 4807.541. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1000. time since epoch: 4855.450. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1010. time since epoch: 4903.954. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1020. time since epoch: 4952.174. Train acc: 0.882. Train Loss: 0.304\n",
      "Step 1030. time since epoch: 5000.326. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1040. time since epoch: 5048.660. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1050. time since epoch: 5096.733. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1060. time since epoch: 5145.294. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1070. time since epoch: 5193.475. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1080. time since epoch: 5242.211. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1090. time since epoch: 5291.780. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1100. time since epoch: 5339.998. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1110. time since epoch: 5388.387. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1120. time since epoch: 5436.835. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1130. time since epoch: 5485.666. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1140. time since epoch: 5534.955. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1150. time since epoch: 5583.819. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1160. time since epoch: 5632.712. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1170. time since epoch: 5681.567. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1180. time since epoch: 5730.503. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1190. time since epoch: 5778.806. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1200. time since epoch: 5827.339. Train acc: 0.883. Train Loss: 0.304\n",
      "Step 1210. time since epoch: 5875.375. Train acc: 0.883. Train Loss: 0.303\n",
      "Step 1220. time since epoch: 5924.903. Train acc: 0.883. Train Loss: 0.303\n",
      "Step 1230. time since epoch: 5972.788. Train acc: 0.883. Train Loss: 0.303\n",
      "Step 1240. time since epoch: 6020.770. Train acc: 0.883. Train Loss: 0.303\n",
      "Step 1250. time since epoch: 6068.748. Train acc: 0.883. Train Loss: 0.303\n",
      "Step 1260. time since epoch: 6116.862. Train acc: 0.883. Train Loss: 0.303\n",
      "Step 1270. time since epoch: 6164.583. Train acc: 0.883. Train Loss: 0.303\n",
      "Step 1280. time since epoch: 6212.743. Train acc: 0.883. Train Loss: 0.303\n",
      "Step 1290. time since epoch: 6260.600. Train acc: 0.883. Train Loss: 0.303\n",
      "Step 1300. time since epoch: 6308.827. Train acc: 0.883. Train Loss: 0.303\n",
      "Step 1310. time since epoch: 6357.011. Train acc: 0.883. Train Loss: 0.303\n",
      "Step 1320. time since epoch: 6405.111. Train acc: 0.883. Train Loss: 0.303\n",
      "Step 1330. time since epoch: 6453.276. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1340. time since epoch: 6501.540. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1350. time since epoch: 6549.723. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1360. time since epoch: 6598.284. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1370. time since epoch: 6647.221. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1380. time since epoch: 6696.982. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1390. time since epoch: 6746.005. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1400. time since epoch: 6795.649. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1410. time since epoch: 6844.975. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1420. time since epoch: 6894.555. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1430. time since epoch: 6944.392. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1440. time since epoch: 6994.169. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1450. time since epoch: 7044.065. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1460. time since epoch: 7093.969. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1470. time since epoch: 7143.582. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1480. time since epoch: 7193.334. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1490. time since epoch: 7242.981. Train acc: 0.883. Train Loss: 0.302\n",
      "Step 1500. time since epoch: 7293.157. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1510. time since epoch: 7342.943. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1520. time since epoch: 7392.699. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1530. time since epoch: 7441.465. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1540. time since epoch: 7490.052. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1550. time since epoch: 7538.643. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1560. time since epoch: 7587.093. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1570. time since epoch: 7635.770. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1580. time since epoch: 7684.483. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1590. time since epoch: 7734.254. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1600. time since epoch: 7783.657. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1610. time since epoch: 7832.948. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1620. time since epoch: 7882.906. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1630. time since epoch: 7932.862. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1640. time since epoch: 7982.647. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1650. time since epoch: 8032.385. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1660. time since epoch: 8082.087. Train acc: 0.883. Train Loss: 0.301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1670. time since epoch: 8132.041. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1680. time since epoch: 8181.628. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1690. time since epoch: 8231.386. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1700. time since epoch: 8281.263. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1710. time since epoch: 8330.727. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1720. time since epoch: 8379.974. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1730. time since epoch: 8429.853. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1740. time since epoch: 8479.540. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 1750. time since epoch: 8529.403. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 1760. time since epoch: 8578.666. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1770. time since epoch: 8628.107. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 1780. time since epoch: 8677.250. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1790. time since epoch: 8726.019. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1800. time since epoch: 8774.747. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1810. time since epoch: 8823.493. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1820. time since epoch: 8872.198. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1830. time since epoch: 8920.320. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 1840. time since epoch: 8968.677. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1850. time since epoch: 9016.856. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1860. time since epoch: 9065.046. Train acc: 0.883. Train Loss: 0.301\n",
      "Step 1870. time since epoch: 9113.085. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 1880. time since epoch: 9161.834. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 1890. time since epoch: 9211.685. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 1900. time since epoch: 9261.315. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 1910. time since epoch: 9311.019. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 1920. time since epoch: 9360.417. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 1930. time since epoch: 9410.646. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 1940. time since epoch: 9460.152. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 1950. time since epoch: 9509.501. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 1960. time since epoch: 9559.164. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 1970. time since epoch: 9608.493. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 1980. time since epoch: 9658.650. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 1990. time since epoch: 9707.364. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 2000. time since epoch: 9755.796. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 2010. time since epoch: 9804.610. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 2020. time since epoch: 9854.317. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 2030. time since epoch: 9904.099. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 2040. time since epoch: 9953.566. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 2050. time since epoch: 10003.016. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 2060. time since epoch: 10052.552. Train acc: 0.883. Train Loss: 0.300\n",
      "Step 2070. time since epoch: 10102.337. Train acc: 0.883. Train Loss: 0.299\n",
      "Step 2080. time since epoch: 10150.697. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2090. time since epoch: 10199.237. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2100. time since epoch: 10247.939. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2110. time since epoch: 10296.676. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2120. time since epoch: 10345.875. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2130. time since epoch: 10395.486. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2140. time since epoch: 10443.953. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2150. time since epoch: 10492.468. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2160. time since epoch: 10540.974. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2170. time since epoch: 10590.745. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2180. time since epoch: 10640.157. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2190. time since epoch: 10688.827. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2200. time since epoch: 10737.434. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2210. time since epoch: 10786.355. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2220. time since epoch: 10836.342. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2230. time since epoch: 10885.953. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2240. time since epoch: 10935.514. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2250. time since epoch: 10985.381. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2260. time since epoch: 11034.908. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2270. time since epoch: 11084.796. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2280. time since epoch: 11134.696. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2290. time since epoch: 11184.265. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2300. time since epoch: 11233.549. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2310. time since epoch: 11282.800. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2320. time since epoch: 11332.556. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2330. time since epoch: 11381.913. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2340. time since epoch: 11431.200. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2350. time since epoch: 11479.766. Train acc: 0.884. Train Loss: 0.299\n",
      "Step 2360. time since epoch: 11528.265. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2370. time since epoch: 11576.945. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2380. time since epoch: 11625.615. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2390. time since epoch: 11674.110. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2400. time since epoch: 11722.710. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2410. time since epoch: 11771.264. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2420. time since epoch: 11819.568. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2430. time since epoch: 11868.665. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2440. time since epoch: 11917.711. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2450. time since epoch: 11966.622. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2460. time since epoch: 12015.490. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2470. time since epoch: 12064.444. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2480. time since epoch: 12113.039. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2490. time since epoch: 12161.332. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2500. time since epoch: 12209.835. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2510. time since epoch: 12257.949. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2520. time since epoch: 12306.533. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2530. time since epoch: 12355.391. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2540. time since epoch: 12404.736. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2550. time since epoch: 12454.081. Train acc: 0.884. Train Loss: 0.298\n",
      "Step 2560. time since epoch: 12504.127. Train acc: 0.884. Train Loss: 0.297\n",
      "Step 2570. time since epoch: 12553.061. Train acc: 0.884. Train Loss: 0.297\n",
      "Step 2580. time since epoch: 12601.857. Train acc: 0.884. Train Loss: 0.297\n",
      "Step 2590. time since epoch: 12650.626. Train acc: 0.884. Train Loss: 0.297\n",
      "Step 2600. time since epoch: 12699.596. Train acc: 0.884. Train Loss: 0.297\n",
      "Step 2610. time since epoch: 12748.992. Train acc: 0.884. Train Loss: 0.297\n",
      "Step 2620. time since epoch: 12798.238. Train acc: 0.884. Train Loss: 0.297\n",
      "Step 2630. time since epoch: 12846.790. Train acc: 0.884. Train Loss: 0.297\n",
      "Step 2640. time since epoch: 12894.869. Train acc: 0.884. Train Loss: 0.297\n",
      "Step 2650. time since epoch: 12942.641. Train acc: 0.884. Train Loss: 0.297\n",
      "Step 2660. time since epoch: 12990.745. Train acc: 0.884. Train Loss: 0.297\n",
      "Step 2670. time since epoch: 13039.193. Train acc: 0.884. Train Loss: 0.297\n",
      "Step 2680. time since epoch: 13087.283. Train acc: 0.884. Train Loss: 0.297\n",
      "Step 2690. time since epoch: 13135.042. Train acc: 0.884. Train Loss: 0.297\n",
      "Step 2700. time since epoch: 13182.979. Train acc: 0.884. Train Loss: 0.297\n",
      "Step 2710. time since epoch: 13230.841. Train acc: 0.884. Train Loss: 0.296\n",
      "Step 2720. time since epoch: 13278.907. Train acc: 0.884. Train Loss: 0.297\n",
      "--------------------\n",
      "epoch 4, loss 0.2965, train acc 0.884, test acc 0.867, time 14183.1 sec\n",
      "Step 0. time since epoch: 4.874. Train acc: 0.906. Train Loss: 0.221\n",
      "Step 10. time since epoch: 54.604. Train acc: 0.882. Train Loss: 0.299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20. time since epoch: 104.567. Train acc: 0.883. Train Loss: 0.298\n",
      "Step 30. time since epoch: 154.391. Train acc: 0.884. Train Loss: 0.291\n",
      "Step 40. time since epoch: 204.086. Train acc: 0.881. Train Loss: 0.296\n",
      "Step 50. time since epoch: 253.854. Train acc: 0.883. Train Loss: 0.291\n",
      "Step 60. time since epoch: 303.707. Train acc: 0.883. Train Loss: 0.294\n",
      "Step 70. time since epoch: 353.734. Train acc: 0.884. Train Loss: 0.293\n",
      "Step 80. time since epoch: 403.882. Train acc: 0.886. Train Loss: 0.290\n",
      "Step 90. time since epoch: 453.644. Train acc: 0.886. Train Loss: 0.289\n",
      "Step 100. time since epoch: 503.295. Train acc: 0.888. Train Loss: 0.286\n",
      "Step 110. time since epoch: 552.233. Train acc: 0.888. Train Loss: 0.284\n",
      "Step 120. time since epoch: 601.030. Train acc: 0.888. Train Loss: 0.283\n",
      "Step 130. time since epoch: 649.987. Train acc: 0.888. Train Loss: 0.284\n",
      "Step 140. time since epoch: 698.264. Train acc: 0.888. Train Loss: 0.284\n",
      "Step 150. time since epoch: 746.986. Train acc: 0.887. Train Loss: 0.285\n",
      "Step 160. time since epoch: 795.415. Train acc: 0.888. Train Loss: 0.285\n",
      "Step 170. time since epoch: 843.889. Train acc: 0.888. Train Loss: 0.284\n",
      "Step 180. time since epoch: 892.650. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 190. time since epoch: 941.086. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 200. time since epoch: 989.413. Train acc: 0.889. Train Loss: 0.282\n",
      "Step 210. time since epoch: 1037.637. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 220. time since epoch: 1086.114. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 230. time since epoch: 1134.505. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 240. time since epoch: 1183.254. Train acc: 0.889. Train Loss: 0.284\n",
      "Step 250. time since epoch: 1231.224. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 260. time since epoch: 1279.841. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 270. time since epoch: 1328.396. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 280. time since epoch: 1376.793. Train acc: 0.890. Train Loss: 0.282\n",
      "Step 290. time since epoch: 1425.168. Train acc: 0.889. Train Loss: 0.282\n",
      "Step 300. time since epoch: 1473.536. Train acc: 0.889. Train Loss: 0.282\n",
      "Step 310. time since epoch: 1522.358. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 320. time since epoch: 1572.103. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 330. time since epoch: 1620.992. Train acc: 0.890. Train Loss: 0.283\n",
      "Step 340. time since epoch: 1670.232. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 350. time since epoch: 1719.528. Train acc: 0.889. Train Loss: 0.284\n",
      "Step 360. time since epoch: 1769.076. Train acc: 0.889. Train Loss: 0.284\n",
      "Step 370. time since epoch: 1818.530. Train acc: 0.889. Train Loss: 0.284\n",
      "Step 380. time since epoch: 1868.144. Train acc: 0.889. Train Loss: 0.284\n",
      "Step 390. time since epoch: 1918.169. Train acc: 0.889. Train Loss: 0.284\n",
      "Step 400. time since epoch: 1967.950. Train acc: 0.888. Train Loss: 0.285\n",
      "Step 410. time since epoch: 2017.837. Train acc: 0.888. Train Loss: 0.285\n",
      "Step 420. time since epoch: 2067.544. Train acc: 0.888. Train Loss: 0.285\n",
      "Step 430. time since epoch: 2117.433. Train acc: 0.888. Train Loss: 0.285\n",
      "Step 440. time since epoch: 2167.190. Train acc: 0.888. Train Loss: 0.285\n",
      "Step 450. time since epoch: 2217.247. Train acc: 0.888. Train Loss: 0.285\n",
      "Step 460. time since epoch: 2267.020. Train acc: 0.888. Train Loss: 0.285\n",
      "Step 470. time since epoch: 2316.447. Train acc: 0.888. Train Loss: 0.285\n",
      "Step 480. time since epoch: 2366.132. Train acc: 0.889. Train Loss: 0.284\n",
      "Step 490. time since epoch: 2415.401. Train acc: 0.889. Train Loss: 0.284\n",
      "Step 500. time since epoch: 2465.505. Train acc: 0.889. Train Loss: 0.284\n",
      "Step 510. time since epoch: 2514.505. Train acc: 0.889. Train Loss: 0.284\n",
      "Step 520. time since epoch: 2563.134. Train acc: 0.889. Train Loss: 0.284\n",
      "Step 530. time since epoch: 2611.696. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 540. time since epoch: 2661.158. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 550. time since epoch: 2711.105. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 560. time since epoch: 2760.900. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 570. time since epoch: 2810.621. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 580. time since epoch: 2859.583. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 590. time since epoch: 2908.375. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 600. time since epoch: 2957.159. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 610. time since epoch: 3006.734. Train acc: 0.889. Train Loss: 0.283\n",
      "Step 620. time since epoch: 3056.432. Train acc: 0.889. Train Loss: 0.282\n",
      "Step 630. time since epoch: 3105.924. Train acc: 0.889. Train Loss: 0.282\n",
      "Step 640. time since epoch: 3155.869. Train acc: 0.889. Train Loss: 0.282\n",
      "Step 650. time since epoch: 3205.888. Train acc: 0.890. Train Loss: 0.281\n",
      "Step 660. time since epoch: 3255.882. Train acc: 0.890. Train Loss: 0.281\n",
      "Step 670. time since epoch: 3305.849. Train acc: 0.890. Train Loss: 0.281\n",
      "Step 680. time since epoch: 3355.935. Train acc: 0.890. Train Loss: 0.281\n",
      "Step 690. time since epoch: 3405.529. Train acc: 0.890. Train Loss: 0.281\n",
      "Step 700. time since epoch: 3455.318. Train acc: 0.890. Train Loss: 0.281\n",
      "Step 710. time since epoch: 3505.279. Train acc: 0.890. Train Loss: 0.281\n",
      "Step 720. time since epoch: 3554.738. Train acc: 0.890. Train Loss: 0.281\n",
      "Step 730. time since epoch: 3603.685. Train acc: 0.890. Train Loss: 0.281\n",
      "Step 740. time since epoch: 3652.518. Train acc: 0.890. Train Loss: 0.281\n",
      "Step 750. time since epoch: 3702.249. Train acc: 0.890. Train Loss: 0.281\n",
      "Step 760. time since epoch: 3751.353. Train acc: 0.890. Train Loss: 0.281\n",
      "Step 770. time since epoch: 3800.332. Train acc: 0.890. Train Loss: 0.281\n",
      "Step 780. time since epoch: 3850.100. Train acc: 0.890. Train Loss: 0.280\n",
      "Step 790. time since epoch: 3899.965. Train acc: 0.890. Train Loss: 0.281\n",
      "Step 800. time since epoch: 3949.981. Train acc: 0.890. Train Loss: 0.281\n",
      "Step 810. time since epoch: 4000.179. Train acc: 0.890. Train Loss: 0.281\n",
      "Step 820. time since epoch: 4048.974. Train acc: 0.890. Train Loss: 0.280\n",
      "Step 830. time since epoch: 4097.767. Train acc: 0.890. Train Loss: 0.280\n",
      "Step 840. time since epoch: 4146.689. Train acc: 0.890. Train Loss: 0.280\n",
      "Step 850. time since epoch: 4195.263. Train acc: 0.890. Train Loss: 0.280\n",
      "Step 860. time since epoch: 4244.791. Train acc: 0.890. Train Loss: 0.280\n",
      "Step 870. time since epoch: 4294.259. Train acc: 0.890. Train Loss: 0.280\n",
      "Step 880. time since epoch: 4342.868. Train acc: 0.890. Train Loss: 0.279\n",
      "Step 890. time since epoch: 4391.776. Train acc: 0.890. Train Loss: 0.279\n",
      "Step 900. time since epoch: 4440.286. Train acc: 0.890. Train Loss: 0.279\n",
      "Step 910. time since epoch: 4489.085. Train acc: 0.890. Train Loss: 0.279\n",
      "Step 920. time since epoch: 4537.669. Train acc: 0.890. Train Loss: 0.279\n",
      "Step 930. time since epoch: 4586.262. Train acc: 0.890. Train Loss: 0.279\n",
      "Step 940. time since epoch: 4635.260. Train acc: 0.890. Train Loss: 0.279\n",
      "Step 950. time since epoch: 4683.902. Train acc: 0.891. Train Loss: 0.278\n",
      "Step 960. time since epoch: 4732.717. Train acc: 0.891. Train Loss: 0.278\n",
      "Step 970. time since epoch: 4781.587. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 980. time since epoch: 4830.359. Train acc: 0.891. Train Loss: 0.278\n",
      "Step 990. time since epoch: 4879.177. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1000. time since epoch: 4927.998. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1010. time since epoch: 4976.802. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1020. time since epoch: 5025.048. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1030. time since epoch: 5073.644. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1040. time since epoch: 5122.305. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1050. time since epoch: 5171.525. Train acc: 0.891. Train Loss: 0.278\n",
      "Step 1060. time since epoch: 5219.965. Train acc: 0.891. Train Loss: 0.278\n",
      "Step 1070. time since epoch: 5268.061. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1080. time since epoch: 5316.014. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1090. time since epoch: 5364.102. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1100. time since epoch: 5412.952. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1110. time since epoch: 5461.664. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1120. time since epoch: 5510.886. Train acc: 0.890. Train Loss: 0.278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1130. time since epoch: 5559.539. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1140. time since epoch: 5609.649. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1150. time since epoch: 5658.379. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1160. time since epoch: 5707.139. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1170. time since epoch: 5755.969. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1180. time since epoch: 5805.131. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1190. time since epoch: 5854.445. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1200. time since epoch: 5903.446. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1210. time since epoch: 5951.906. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1220. time since epoch: 6000.441. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1230. time since epoch: 6049.015. Train acc: 0.890. Train Loss: 0.278\n",
      "Step 1240. time since epoch: 6097.540. Train acc: 0.891. Train Loss: 0.278\n",
      "Step 1250. time since epoch: 6145.491. Train acc: 0.891. Train Loss: 0.278\n",
      "Step 1260. time since epoch: 6193.337. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1270. time since epoch: 6241.391. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1280. time since epoch: 6289.445. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1290. time since epoch: 6337.796. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1300. time since epoch: 6385.554. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1310. time since epoch: 6433.846. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1320. time since epoch: 6481.754. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1330. time since epoch: 6529.848. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1340. time since epoch: 6577.886. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1350. time since epoch: 6625.916. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1360. time since epoch: 6673.976. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1370. time since epoch: 6722.213. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1380. time since epoch: 6770.342. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1390. time since epoch: 6818.263. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1400. time since epoch: 6866.837. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1410. time since epoch: 6915.583. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1420. time since epoch: 6964.499. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1430. time since epoch: 7014.916. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1440. time since epoch: 7064.522. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1450. time since epoch: 7114.114. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1460. time since epoch: 7164.215. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1470. time since epoch: 7213.974. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1480. time since epoch: 7263.651. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1490. time since epoch: 7313.510. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1500. time since epoch: 7363.398. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1510. time since epoch: 7413.254. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1520. time since epoch: 7463.100. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1530. time since epoch: 7512.973. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1540. time since epoch: 7562.873. Train acc: 0.891. Train Loss: 0.277\n",
      "Step 1550. time since epoch: 7612.849. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1560. time since epoch: 7662.633. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1570. time since epoch: 7712.459. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1580. time since epoch: 7761.808. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1590. time since epoch: 7810.690. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1600. time since epoch: 7859.596. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1610. time since epoch: 7908.610. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1620. time since epoch: 7957.234. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1630. time since epoch: 8006.553. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1640. time since epoch: 8056.357. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1650. time since epoch: 8106.250. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1660. time since epoch: 8155.677. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1670. time since epoch: 8205.781. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1680. time since epoch: 8255.064. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1690. time since epoch: 8304.797. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1700. time since epoch: 8354.280. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1710. time since epoch: 8403.033. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1720. time since epoch: 8451.862. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1730. time since epoch: 8500.577. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1740. time since epoch: 8549.333. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1750. time since epoch: 8598.146. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1760. time since epoch: 8646.782. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1770. time since epoch: 8695.454. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1780. time since epoch: 8745.562. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1790. time since epoch: 8795.403. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1800. time since epoch: 8844.472. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1810. time since epoch: 8893.058. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1820. time since epoch: 8942.320. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1830. time since epoch: 8992.164. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1840. time since epoch: 9042.044. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1850. time since epoch: 9091.684. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1860. time since epoch: 9141.558. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1870. time since epoch: 9191.472. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1880. time since epoch: 9241.229. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1890. time since epoch: 9290.948. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1900. time since epoch: 9340.798. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1910. time since epoch: 9390.578. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1920. time since epoch: 9440.861. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1930. time since epoch: 9492.300. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1940. time since epoch: 9542.338. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1950. time since epoch: 9592.089. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1960. time since epoch: 9641.640. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1970. time since epoch: 9690.786. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 1980. time since epoch: 9739.774. Train acc: 0.891. Train Loss: 0.275\n",
      "Step 1990. time since epoch: 9788.610. Train acc: 0.891. Train Loss: 0.276\n",
      "Step 2000. time since epoch: 9838.967. Train acc: 0.891. Train Loss: 0.275\n",
      "Step 2010. time since epoch: 9887.351. Train acc: 0.891. Train Loss: 0.275\n",
      "Step 2020. time since epoch: 9936.374. Train acc: 0.891. Train Loss: 0.275\n",
      "Step 2030. time since epoch: 9985.121. Train acc: 0.891. Train Loss: 0.275\n",
      "Step 2040. time since epoch: 10034.777. Train acc: 0.891. Train Loss: 0.275\n",
      "Step 2050. time since epoch: 10084.161. Train acc: 0.891. Train Loss: 0.275\n",
      "Step 2060. time since epoch: 10133.302. Train acc: 0.891. Train Loss: 0.275\n",
      "Step 2070. time since epoch: 10182.455. Train acc: 0.891. Train Loss: 0.275\n",
      "Step 2080. time since epoch: 10231.836. Train acc: 0.891. Train Loss: 0.275\n",
      "Step 2090. time since epoch: 10281.328. Train acc: 0.891. Train Loss: 0.275\n",
      "Step 2100. time since epoch: 10330.784. Train acc: 0.891. Train Loss: 0.275\n",
      "Step 2110. time since epoch: 10379.868. Train acc: 0.891. Train Loss: 0.275\n",
      "Step 2120. time since epoch: 10429.072. Train acc: 0.891. Train Loss: 0.275\n",
      "Step 2130. time since epoch: 10478.689. Train acc: 0.891. Train Loss: 0.275\n",
      "Step 2140. time since epoch: 10527.962. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2150. time since epoch: 10577.713. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2160. time since epoch: 10627.411. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2170. time since epoch: 10676.858. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2180. time since epoch: 10726.603. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2190. time since epoch: 10776.411. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2200. time since epoch: 10825.863. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2210. time since epoch: 10875.290. Train acc: 0.891. Train Loss: 0.274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2220. time since epoch: 10924.479. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2230. time since epoch: 10973.792. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2240. time since epoch: 11023.253. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2250. time since epoch: 11072.546. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2260. time since epoch: 11121.742. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2270. time since epoch: 11171.357. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2280. time since epoch: 11221.108. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2290. time since epoch: 11269.528. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2300. time since epoch: 11318.671. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2310. time since epoch: 11368.212. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2320. time since epoch: 11417.600. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2330. time since epoch: 11466.885. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2340. time since epoch: 11516.564. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2350. time since epoch: 11566.254. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2360. time since epoch: 11615.043. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2370. time since epoch: 11663.931. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2380. time since epoch: 11713.079. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2390. time since epoch: 11762.285. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2400. time since epoch: 11811.351. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2410. time since epoch: 11859.936. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2420. time since epoch: 11909.353. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2430. time since epoch: 11957.846. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2440. time since epoch: 12007.222. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2450. time since epoch: 12056.799. Train acc: 0.892. Train Loss: 0.274\n",
      "Step 2460. time since epoch: 12106.680. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2470. time since epoch: 12154.727. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2480. time since epoch: 12203.554. Train acc: 0.891. Train Loss: 0.274\n",
      "Step 2490. time since epoch: 12252.819. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2500. time since epoch: 12302.061. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2510. time since epoch: 12351.327. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2520. time since epoch: 12400.787. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2530. time since epoch: 12449.655. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2540. time since epoch: 12498.391. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2550. time since epoch: 12547.487. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2560. time since epoch: 12596.421. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2570. time since epoch: 12645.442. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2580. time since epoch: 12694.785. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2590. time since epoch: 12743.689. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2600. time since epoch: 12792.110. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2610. time since epoch: 12841.692. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2620. time since epoch: 12890.206. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2630. time since epoch: 12938.720. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2640. time since epoch: 12986.873. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2650. time since epoch: 13035.258. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2660. time since epoch: 13083.978. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2670. time since epoch: 13132.335. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2680. time since epoch: 13180.735. Train acc: 0.892. Train Loss: 0.273\n",
      "Step 2690. time since epoch: 13229.737. Train acc: 0.892. Train Loss: 0.272\n",
      "Step 2700. time since epoch: 13278.909. Train acc: 0.892. Train Loss: 0.272\n",
      "Step 2710. time since epoch: 13328.652. Train acc: 0.892. Train Loss: 0.272\n",
      "Step 2720. time since epoch: 13378.673. Train acc: 0.892. Train Loss: 0.272\n",
      "--------------------\n",
      "epoch 5, loss 0.2724, train acc 0.892, test acc 0.864, time 14317.6 sec\n",
      "Step 0. time since epoch: 4.979. Train acc: 0.898. Train Loss: 0.222\n",
      "Step 10. time since epoch: 54.171. Train acc: 0.889. Train Loss: 0.272\n",
      "Step 20. time since epoch: 103.553. Train acc: 0.888. Train Loss: 0.273\n",
      "Step 30. time since epoch: 153.003. Train acc: 0.889. Train Loss: 0.269\n",
      "Step 40. time since epoch: 202.387. Train acc: 0.888. Train Loss: 0.273\n",
      "Step 50. time since epoch: 251.699. Train acc: 0.890. Train Loss: 0.267\n",
      "Step 60. time since epoch: 300.516. Train acc: 0.890. Train Loss: 0.269\n",
      "Step 70. time since epoch: 350.039. Train acc: 0.891. Train Loss: 0.267\n",
      "Step 80. time since epoch: 399.293. Train acc: 0.893. Train Loss: 0.264\n",
      "Step 90. time since epoch: 448.489. Train acc: 0.893. Train Loss: 0.264\n",
      "Step 100. time since epoch: 497.582. Train acc: 0.895. Train Loss: 0.261\n",
      "Step 110. time since epoch: 545.632. Train acc: 0.895. Train Loss: 0.260\n",
      "Step 120. time since epoch: 594.789. Train acc: 0.895. Train Loss: 0.259\n",
      "Step 130. time since epoch: 644.085. Train acc: 0.895. Train Loss: 0.259\n",
      "Step 140. time since epoch: 693.559. Train acc: 0.895. Train Loss: 0.260\n",
      "Step 150. time since epoch: 742.663. Train acc: 0.895. Train Loss: 0.261\n",
      "Step 160. time since epoch: 792.163. Train acc: 0.895. Train Loss: 0.261\n",
      "Step 170. time since epoch: 841.643. Train acc: 0.895. Train Loss: 0.261\n",
      "Step 180. time since epoch: 891.233. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 190. time since epoch: 940.316. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 200. time since epoch: 989.346. Train acc: 0.896. Train Loss: 0.259\n",
      "Step 210. time since epoch: 1038.382. Train acc: 0.896. Train Loss: 0.259\n",
      "Step 220. time since epoch: 1087.429. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 230. time since epoch: 1136.489. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 240. time since epoch: 1185.530. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 250. time since epoch: 1235.172. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 260. time since epoch: 1284.104. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 270. time since epoch: 1332.861. Train acc: 0.897. Train Loss: 0.259\n",
      "Step 280. time since epoch: 1381.169. Train acc: 0.897. Train Loss: 0.259\n",
      "Step 290. time since epoch: 1431.404. Train acc: 0.897. Train Loss: 0.259\n",
      "Step 300. time since epoch: 1480.233. Train acc: 0.897. Train Loss: 0.259\n",
      "Step 310. time since epoch: 1529.763. Train acc: 0.897. Train Loss: 0.259\n",
      "Step 320. time since epoch: 1579.633. Train acc: 0.897. Train Loss: 0.259\n",
      "Step 330. time since epoch: 1629.300. Train acc: 0.897. Train Loss: 0.259\n",
      "Step 340. time since epoch: 1679.128. Train acc: 0.897. Train Loss: 0.260\n",
      "Step 350. time since epoch: 1729.210. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 360. time since epoch: 1779.215. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 370. time since epoch: 1828.315. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 380. time since epoch: 1877.077. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 390. time since epoch: 1926.315. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 400. time since epoch: 1975.648. Train acc: 0.896. Train Loss: 0.261\n",
      "Step 410. time since epoch: 2025.275. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 420. time since epoch: 2074.543. Train acc: 0.895. Train Loss: 0.261\n",
      "Step 430. time since epoch: 2123.774. Train acc: 0.895. Train Loss: 0.261\n",
      "Step 440. time since epoch: 2173.167. Train acc: 0.895. Train Loss: 0.261\n",
      "Step 450. time since epoch: 2222.488. Train acc: 0.895. Train Loss: 0.261\n",
      "Step 460. time since epoch: 2272.021. Train acc: 0.895. Train Loss: 0.261\n",
      "Step 470. time since epoch: 2321.656. Train acc: 0.895. Train Loss: 0.261\n",
      "Step 480. time since epoch: 2371.003. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 490. time since epoch: 2420.435. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 500. time since epoch: 2469.812. Train acc: 0.895. Train Loss: 0.260\n",
      "Step 510. time since epoch: 2519.095. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 520. time since epoch: 2568.693. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 530. time since epoch: 2617.811. Train acc: 0.896. Train Loss: 0.260\n",
      "Step 540. time since epoch: 2666.468. Train acc: 0.896. Train Loss: 0.259\n",
      "Step 550. time since epoch: 2715.180. Train acc: 0.896. Train Loss: 0.259\n",
      "Step 560. time since epoch: 2763.418. Train acc: 0.896. Train Loss: 0.259\n",
      "Step 570. time since epoch: 2811.548. Train acc: 0.896. Train Loss: 0.259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 580. time since epoch: 2859.844. Train acc: 0.896. Train Loss: 0.259\n",
      "Step 590. time since epoch: 2908.292. Train acc: 0.896. Train Loss: 0.259\n",
      "Step 600. time since epoch: 2956.796. Train acc: 0.896. Train Loss: 0.259\n",
      "Step 610. time since epoch: 3006.004. Train acc: 0.896. Train Loss: 0.259\n",
      "Step 620. time since epoch: 3055.141. Train acc: 0.896. Train Loss: 0.259\n",
      "Step 630. time since epoch: 3103.627. Train acc: 0.896. Train Loss: 0.259\n",
      "Step 640. time since epoch: 3152.950. Train acc: 0.896. Train Loss: 0.259\n",
      "Step 650. time since epoch: 3202.526. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 660. time since epoch: 3251.696. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 670. time since epoch: 3301.091. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 680. time since epoch: 3350.344. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 690. time since epoch: 3399.627. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 700. time since epoch: 3448.941. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 710. time since epoch: 3498.196. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 720. time since epoch: 3546.395. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 730. time since epoch: 3596.025. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 740. time since epoch: 3645.041. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 750. time since epoch: 3693.898. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 760. time since epoch: 3743.077. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 770. time since epoch: 3791.972. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 780. time since epoch: 3841.148. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 790. time since epoch: 3890.403. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 800. time since epoch: 3939.826. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 810. time since epoch: 3989.089. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 820. time since epoch: 4038.682. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 830. time since epoch: 4087.992. Train acc: 0.896. Train Loss: 0.258\n",
      "Step 840. time since epoch: 4136.406. Train acc: 0.897. Train Loss: 0.258\n",
      "Step 850. time since epoch: 4185.795. Train acc: 0.897. Train Loss: 0.257\n",
      "Step 860. time since epoch: 4235.335. Train acc: 0.897. Train Loss: 0.257\n",
      "Step 870. time since epoch: 4284.834. Train acc: 0.897. Train Loss: 0.257\n",
      "Step 880. time since epoch: 4334.367. Train acc: 0.897. Train Loss: 0.257\n",
      "Step 890. time since epoch: 4383.762. Train acc: 0.897. Train Loss: 0.257\n",
      "Step 900. time since epoch: 4432.987. Train acc: 0.897. Train Loss: 0.257\n",
      "Step 910. time since epoch: 4482.155. Train acc: 0.897. Train Loss: 0.257\n",
      "Step 920. time since epoch: 4530.891. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 930. time since epoch: 4578.910. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 940. time since epoch: 4626.509. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 950. time since epoch: 4674.564. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 960. time since epoch: 4722.790. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 970. time since epoch: 4770.849. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 980. time since epoch: 4818.979. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 990. time since epoch: 4866.925. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1000. time since epoch: 4914.850. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1010. time since epoch: 4963.030. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1020. time since epoch: 5011.448. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1030. time since epoch: 5059.375. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1040. time since epoch: 5107.960. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1050. time since epoch: 5156.728. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1060. time since epoch: 5205.905. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1070. time since epoch: 5255.112. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1080. time since epoch: 5304.975. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1090. time since epoch: 5354.880. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1100. time since epoch: 5404.927. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1110. time since epoch: 5454.391. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1120. time since epoch: 5503.650. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1130. time since epoch: 5552.955. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1140. time since epoch: 5602.389. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1150. time since epoch: 5652.029. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1160. time since epoch: 5701.367. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1170. time since epoch: 5750.900. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1180. time since epoch: 5800.800. Train acc: 0.897. Train Loss: 0.257\n",
      "Step 1190. time since epoch: 5849.496. Train acc: 0.897. Train Loss: 0.257\n",
      "Step 1200. time since epoch: 5898.304. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1210. time since epoch: 5947.327. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1220. time since epoch: 5996.435. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1230. time since epoch: 6045.226. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1240. time since epoch: 6093.691. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1250. time since epoch: 6142.589. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1260. time since epoch: 6191.449. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1270. time since epoch: 6240.203. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1280. time since epoch: 6288.840. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1290. time since epoch: 6336.907. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1300. time since epoch: 6384.843. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1310. time since epoch: 6432.729. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1320. time since epoch: 6480.389. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1330. time since epoch: 6528.734. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1340. time since epoch: 6577.004. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1350. time since epoch: 6625.629. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1360. time since epoch: 6674.682. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1370. time since epoch: 6724.276. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1380. time since epoch: 6773.313. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1390. time since epoch: 6821.878. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1400. time since epoch: 6870.281. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1410. time since epoch: 6918.480. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1420. time since epoch: 6966.707. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1430. time since epoch: 7015.130. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1440. time since epoch: 7063.660. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1450. time since epoch: 7112.872. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1460. time since epoch: 7161.654. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1470. time since epoch: 7211.010. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1480. time since epoch: 7259.281. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1490. time since epoch: 7307.746. Train acc: 0.897. Train Loss: 0.256\n",
      "Step 1500. time since epoch: 7356.221. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1510. time since epoch: 7405.256. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1520. time since epoch: 7454.599. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1530. time since epoch: 7503.519. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1540. time since epoch: 7552.730. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1550. time since epoch: 7602.292. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1560. time since epoch: 7651.871. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1570. time since epoch: 7701.486. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1580. time since epoch: 7750.925. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1590. time since epoch: 7800.843. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1600. time since epoch: 7850.624. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1610. time since epoch: 7900.152. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1620. time since epoch: 7949.514. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1630. time since epoch: 7999.209. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1640. time since epoch: 8048.746. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1650. time since epoch: 8097.789. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1660. time since epoch: 8147.089. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1670. time since epoch: 8196.681. Train acc: 0.897. Train Loss: 0.255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1680. time since epoch: 8245.853. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1690. time since epoch: 8295.395. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1700. time since epoch: 8344.898. Train acc: 0.898. Train Loss: 0.255\n",
      "Step 1710. time since epoch: 8394.268. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1720. time since epoch: 8443.749. Train acc: 0.898. Train Loss: 0.255\n",
      "Step 1730. time since epoch: 8493.035. Train acc: 0.898. Train Loss: 0.255\n",
      "Step 1740. time since epoch: 8542.173. Train acc: 0.898. Train Loss: 0.255\n",
      "Step 1750. time since epoch: 8591.571. Train acc: 0.898. Train Loss: 0.255\n",
      "Step 1760. time since epoch: 8640.951. Train acc: 0.898. Train Loss: 0.255\n",
      "Step 1770. time since epoch: 8690.435. Train acc: 0.898. Train Loss: 0.255\n",
      "Step 1780. time since epoch: 8740.192. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1790. time since epoch: 8789.895. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1800. time since epoch: 8839.574. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1810. time since epoch: 8888.849. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1820. time since epoch: 8937.907. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1830. time since epoch: 8987.342. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1840. time since epoch: 9036.594. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1850. time since epoch: 9086.167. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1860. time since epoch: 9135.798. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1870. time since epoch: 9185.195. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1880. time since epoch: 9234.824. Train acc: 0.897. Train Loss: 0.255\n",
      "Step 1890. time since epoch: 9284.345. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 1900. time since epoch: 9333.643. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 1910. time since epoch: 9383.099. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 1920. time since epoch: 9432.844. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 1930. time since epoch: 9481.913. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 1940. time since epoch: 9530.908. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 1950. time since epoch: 9579.353. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 1960. time since epoch: 9627.653. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 1970. time since epoch: 9676.233. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 1980. time since epoch: 9724.776. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 1990. time since epoch: 9773.349. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 2000. time since epoch: 9822.210. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 2010. time since epoch: 9870.791. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 2020. time since epoch: 9919.421. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 2030. time since epoch: 9968.299. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 2040. time since epoch: 10017.020. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 2050. time since epoch: 10065.516. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 2060. time since epoch: 10116.807. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 2070. time since epoch: 10165.260. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 2080. time since epoch: 10213.333. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 2090. time since epoch: 10261.218. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 2100. time since epoch: 10309.043. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 2110. time since epoch: 10357.045. Train acc: 0.898. Train Loss: 0.254\n",
      "Step 2120. time since epoch: 10405.619. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2130. time since epoch: 10455.906. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2140. time since epoch: 10505.403. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2150. time since epoch: 10554.755. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2160. time since epoch: 10604.257. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2170. time since epoch: 10652.786. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2180. time since epoch: 10701.291. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2190. time since epoch: 10749.623. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2200. time since epoch: 10798.233. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2210. time since epoch: 10847.067. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2220. time since epoch: 10894.660. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2230. time since epoch: 10942.409. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2240. time since epoch: 10991.240. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2250. time since epoch: 11040.627. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2260. time since epoch: 11089.863. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2270. time since epoch: 11140.168. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2280. time since epoch: 11189.830. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2290. time since epoch: 11239.645. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2300. time since epoch: 11290.285. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2310. time since epoch: 11340.112. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2320. time since epoch: 11389.713. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2330. time since epoch: 11439.901. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2340. time since epoch: 11489.567. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2350. time since epoch: 11539.618. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2360. time since epoch: 11590.364. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2370. time since epoch: 11640.012. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2380. time since epoch: 11689.245. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2390. time since epoch: 11738.408. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2400. time since epoch: 11785.797. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2410. time since epoch: 11833.549. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2420. time since epoch: 11887.735. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2430. time since epoch: 11941.180. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2440. time since epoch: 11991.165. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2450. time since epoch: 12043.271. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2460. time since epoch: 12095.459. Train acc: 0.898. Train Loss: 0.252\n",
      "Step 2470. time since epoch: 12150.141. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2480. time since epoch: 12215.954. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2490. time since epoch: 12267.755. Train acc: 0.898. Train Loss: 0.253\n",
      "Step 2500. time since epoch: 12328.045. Train acc: 0.898. Train Loss: 0.252\n",
      "Step 2510. time since epoch: 12388.190. Train acc: 0.898. Train Loss: 0.252\n",
      "Step 2520. time since epoch: 12448.504. Train acc: 0.898. Train Loss: 0.252\n",
      "Step 2530. time since epoch: 12501.284. Train acc: 0.899. Train Loss: 0.252\n",
      "Step 2540. time since epoch: 12551.270. Train acc: 0.899. Train Loss: 0.252\n",
      "Step 2550. time since epoch: 12601.872. Train acc: 0.899. Train Loss: 0.252\n",
      "Step 2560. time since epoch: 12652.414. Train acc: 0.899. Train Loss: 0.252\n",
      "Step 2570. time since epoch: 12702.149. Train acc: 0.899. Train Loss: 0.252\n",
      "Step 2580. time since epoch: 12752.596. Train acc: 0.899. Train Loss: 0.252\n",
      "Step 2590. time since epoch: 12801.743. Train acc: 0.899. Train Loss: 0.252\n",
      "Step 2600. time since epoch: 12849.889. Train acc: 0.899. Train Loss: 0.252\n",
      "Step 2610. time since epoch: 12898.429. Train acc: 0.899. Train Loss: 0.252\n",
      "Step 2620. time since epoch: 12947.230. Train acc: 0.899. Train Loss: 0.252\n",
      "Step 2630. time since epoch: 12995.867. Train acc: 0.899. Train Loss: 0.252\n",
      "Step 2640. time since epoch: 13045.550. Train acc: 0.899. Train Loss: 0.252\n",
      "Step 2650. time since epoch: 13094.688. Train acc: 0.899. Train Loss: 0.252\n",
      "Step 2660. time since epoch: 13143.869. Train acc: 0.899. Train Loss: 0.252\n",
      "Step 2670. time since epoch: 13193.056. Train acc: 0.899. Train Loss: 0.252\n",
      "Step 2680. time since epoch: 13242.723. Train acc: 0.899. Train Loss: 0.252\n",
      "Step 2690. time since epoch: 13293.813. Train acc: 0.899. Train Loss: 0.251\n",
      "Step 2700. time since epoch: 13343.683. Train acc: 0.899. Train Loss: 0.251\n",
      "Step 2710. time since epoch: 13392.464. Train acc: 0.899. Train Loss: 0.251\n",
      "Step 2720. time since epoch: 13445.396. Train acc: 0.899. Train Loss: 0.251\n",
      "--------------------\n",
      "epoch 6, loss 0.2512, train acc 0.899, test acc 0.863, time 14533.4 sec\n",
      "Step 0. time since epoch: 6.218. Train acc: 0.926. Train Loss: 0.174\n",
      "Step 10. time since epoch: 64.834. Train acc: 0.898. Train Loss: 0.245\n",
      "Step 20. time since epoch: 124.471. Train acc: 0.900. Train Loss: 0.243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30. time since epoch: 183.668. Train acc: 0.902. Train Loss: 0.243\n",
      "Step 40. time since epoch: 246.449. Train acc: 0.899. Train Loss: 0.249\n",
      "Step 50. time since epoch: 298.786. Train acc: 0.902. Train Loss: 0.245\n",
      "Step 60. time since epoch: 350.966. Train acc: 0.902. Train Loss: 0.245\n",
      "Step 70. time since epoch: 402.519. Train acc: 0.902. Train Loss: 0.244\n",
      "Step 80. time since epoch: 454.888. Train acc: 0.903. Train Loss: 0.241\n",
      "Step 90. time since epoch: 505.500. Train acc: 0.903. Train Loss: 0.240\n",
      "Step 100. time since epoch: 559.958. Train acc: 0.905. Train Loss: 0.236\n",
      "Step 110. time since epoch: 612.394. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 120. time since epoch: 675.565. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 130. time since epoch: 729.211. Train acc: 0.906. Train Loss: 0.235\n",
      "Step 140. time since epoch: 792.272. Train acc: 0.906. Train Loss: 0.235\n",
      "Step 150. time since epoch: 855.681. Train acc: 0.905. Train Loss: 0.237\n",
      "Step 160. time since epoch: 916.882. Train acc: 0.905. Train Loss: 0.236\n",
      "Step 170. time since epoch: 978.430. Train acc: 0.905. Train Loss: 0.236\n",
      "Step 180. time since epoch: 1029.673. Train acc: 0.905. Train Loss: 0.236\n",
      "Step 190. time since epoch: 1086.763. Train acc: 0.905. Train Loss: 0.236\n",
      "Step 200. time since epoch: 1146.387. Train acc: 0.906. Train Loss: 0.235\n",
      "Step 210. time since epoch: 1205.737. Train acc: 0.905. Train Loss: 0.236\n",
      "Step 220. time since epoch: 1264.956. Train acc: 0.905. Train Loss: 0.236\n",
      "Step 230. time since epoch: 1325.521. Train acc: 0.905. Train Loss: 0.236\n",
      "Step 240. time since epoch: 1384.612. Train acc: 0.905. Train Loss: 0.237\n",
      "Step 250. time since epoch: 1436.502. Train acc: 0.905. Train Loss: 0.237\n",
      "Step 260. time since epoch: 1491.302. Train acc: 0.905. Train Loss: 0.237\n",
      "Step 270. time since epoch: 1552.899. Train acc: 0.905. Train Loss: 0.237\n",
      "Step 280. time since epoch: 1602.998. Train acc: 0.906. Train Loss: 0.236\n",
      "Step 290. time since epoch: 1654.263. Train acc: 0.905. Train Loss: 0.237\n",
      "Step 300. time since epoch: 1709.444. Train acc: 0.905. Train Loss: 0.237\n",
      "Step 310. time since epoch: 1763.006. Train acc: 0.905. Train Loss: 0.237\n",
      "Step 320. time since epoch: 1823.351. Train acc: 0.905. Train Loss: 0.237\n",
      "Step 330. time since epoch: 1884.959. Train acc: 0.905. Train Loss: 0.237\n",
      "Step 340. time since epoch: 1949.840. Train acc: 0.905. Train Loss: 0.238\n",
      "Step 350. time since epoch: 2000.187. Train acc: 0.905. Train Loss: 0.238\n",
      "Step 360. time since epoch: 2049.583. Train acc: 0.905. Train Loss: 0.239\n",
      "Step 370. time since epoch: 2098.971. Train acc: 0.905. Train Loss: 0.238\n",
      "Step 380. time since epoch: 2148.095. Train acc: 0.905. Train Loss: 0.238\n",
      "Step 390. time since epoch: 2197.791. Train acc: 0.905. Train Loss: 0.238\n",
      "Step 400. time since epoch: 2247.140. Train acc: 0.905. Train Loss: 0.238\n",
      "Step 410. time since epoch: 2297.314. Train acc: 0.905. Train Loss: 0.238\n",
      "Step 420. time since epoch: 2347.188. Train acc: 0.905. Train Loss: 0.238\n",
      "Step 430. time since epoch: 2395.825. Train acc: 0.905. Train Loss: 0.238\n",
      "Step 440. time since epoch: 2443.833. Train acc: 0.905. Train Loss: 0.238\n",
      "Step 450. time since epoch: 2492.862. Train acc: 0.904. Train Loss: 0.238\n",
      "Step 460. time since epoch: 2541.894. Train acc: 0.904. Train Loss: 0.239\n",
      "Step 470. time since epoch: 2590.509. Train acc: 0.904. Train Loss: 0.239\n",
      "Step 480. time since epoch: 2639.352. Train acc: 0.904. Train Loss: 0.238\n",
      "Step 490. time since epoch: 2688.099. Train acc: 0.904. Train Loss: 0.238\n",
      "Step 500. time since epoch: 2736.887. Train acc: 0.904. Train Loss: 0.238\n",
      "Step 510. time since epoch: 2785.515. Train acc: 0.904. Train Loss: 0.238\n",
      "Step 520. time since epoch: 2833.502. Train acc: 0.904. Train Loss: 0.238\n",
      "Step 530. time since epoch: 2881.640. Train acc: 0.904. Train Loss: 0.238\n",
      "Step 540. time since epoch: 2929.800. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 550. time since epoch: 2978.125. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 560. time since epoch: 3026.739. Train acc: 0.904. Train Loss: 0.238\n",
      "Step 570. time since epoch: 3076.160. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 580. time since epoch: 3125.420. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 590. time since epoch: 3174.457. Train acc: 0.904. Train Loss: 0.238\n",
      "Step 600. time since epoch: 3223.787. Train acc: 0.904. Train Loss: 0.238\n",
      "Step 610. time since epoch: 3272.471. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 620. time since epoch: 3320.374. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 630. time since epoch: 3368.692. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 640. time since epoch: 3416.610. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 650. time since epoch: 3464.494. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 660. time since epoch: 3512.585. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 670. time since epoch: 3560.728. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 680. time since epoch: 3608.805. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 690. time since epoch: 3657.042. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 700. time since epoch: 3705.741. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 710. time since epoch: 3754.200. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 720. time since epoch: 3802.469. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 730. time since epoch: 3851.146. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 740. time since epoch: 3899.731. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 750. time since epoch: 3948.465. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 760. time since epoch: 3996.735. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 770. time since epoch: 4045.123. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 780. time since epoch: 4093.660. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 790. time since epoch: 4142.378. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 800. time since epoch: 4190.342. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 810. time since epoch: 4237.791. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 820. time since epoch: 4286.610. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 830. time since epoch: 4335.289. Train acc: 0.904. Train Loss: 0.237\n",
      "Step 840. time since epoch: 4383.660. Train acc: 0.904. Train Loss: 0.236\n",
      "Step 850. time since epoch: 4432.498. Train acc: 0.904. Train Loss: 0.236\n",
      "Step 860. time since epoch: 4480.894. Train acc: 0.904. Train Loss: 0.236\n",
      "Step 870. time since epoch: 4529.080. Train acc: 0.905. Train Loss: 0.236\n",
      "Step 880. time since epoch: 4577.547. Train acc: 0.905. Train Loss: 0.236\n",
      "Step 890. time since epoch: 4626.444. Train acc: 0.905. Train Loss: 0.236\n",
      "Step 900. time since epoch: 4675.179. Train acc: 0.905. Train Loss: 0.236\n",
      "Step 910. time since epoch: 4724.299. Train acc: 0.905. Train Loss: 0.236\n",
      "Step 920. time since epoch: 4772.378. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 930. time since epoch: 4821.014. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 940. time since epoch: 4870.034. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 950. time since epoch: 4919.189. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 960. time since epoch: 4967.571. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 970. time since epoch: 5016.797. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 980. time since epoch: 5065.657. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 990. time since epoch: 5114.055. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 1000. time since epoch: 5162.590. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 1010. time since epoch: 5211.063. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 1020. time since epoch: 5259.569. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 1030. time since epoch: 5308.496. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 1040. time since epoch: 5357.386. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1050. time since epoch: 5405.867. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1060. time since epoch: 5454.454. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1070. time since epoch: 5503.279. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1080. time since epoch: 5552.181. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 1090. time since epoch: 5600.769. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1100. time since epoch: 5649.453. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 1110. time since epoch: 5698.538. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 1120. time since epoch: 5746.767. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 1130. time since epoch: 5797.821. Train acc: 0.905. Train Loss: 0.235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1140. time since epoch: 5845.519. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 1150. time since epoch: 5894.990. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1160. time since epoch: 5943.164. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1170. time since epoch: 5992.413. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1180. time since epoch: 6040.938. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 1190. time since epoch: 6089.183. Train acc: 0.905. Train Loss: 0.235\n",
      "Step 1200. time since epoch: 6136.979. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1210. time since epoch: 6184.520. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1220. time since epoch: 6232.599. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1230. time since epoch: 6280.339. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1240. time since epoch: 6328.204. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1250. time since epoch: 6376.848. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1260. time since epoch: 6424.888. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1270. time since epoch: 6473.234. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1280. time since epoch: 6521.440. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1290. time since epoch: 6570.500. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1300. time since epoch: 6619.221. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1310. time since epoch: 6668.463. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1320. time since epoch: 6716.718. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1330. time since epoch: 6764.630. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1340. time since epoch: 6812.795. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1350. time since epoch: 6862.488. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1360. time since epoch: 6911.130. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1370. time since epoch: 6959.814. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1380. time since epoch: 7008.958. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1390. time since epoch: 7058.339. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1400. time since epoch: 7106.299. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1410. time since epoch: 7153.881. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1420. time since epoch: 7201.796. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1430. time since epoch: 7249.886. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1440. time since epoch: 7297.829. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1450. time since epoch: 7344.966. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1460. time since epoch: 7392.676. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1470. time since epoch: 7441.191. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1480. time since epoch: 7489.475. Train acc: 0.906. Train Loss: 0.234\n",
      "Step 1490. time since epoch: 7537.519. Train acc: 0.906. Train Loss: 0.234\n",
      "Step 1500. time since epoch: 7586.124. Train acc: 0.906. Train Loss: 0.234\n",
      "Step 1510. time since epoch: 7634.434. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1520. time since epoch: 7683.465. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1530. time since epoch: 7732.505. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1540. time since epoch: 7781.258. Train acc: 0.906. Train Loss: 0.234\n",
      "Step 1550. time since epoch: 7829.349. Train acc: 0.906. Train Loss: 0.234\n",
      "Step 1560. time since epoch: 7877.596. Train acc: 0.906. Train Loss: 0.234\n",
      "Step 1570. time since epoch: 7925.784. Train acc: 0.906. Train Loss: 0.234\n",
      "Step 1580. time since epoch: 7973.852. Train acc: 0.905. Train Loss: 0.234\n",
      "Step 1590. time since epoch: 8023.085. Train acc: 0.906. Train Loss: 0.234\n",
      "Step 1600. time since epoch: 8071.615. Train acc: 0.906. Train Loss: 0.234\n",
      "Step 1610. time since epoch: 8120.003. Train acc: 0.906. Train Loss: 0.234\n",
      "Step 1620. time since epoch: 8168.681. Train acc: 0.906. Train Loss: 0.234\n",
      "Step 1630. time since epoch: 8217.870. Train acc: 0.906. Train Loss: 0.234\n",
      "Step 1640. time since epoch: 8266.241. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1650. time since epoch: 8315.194. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1660. time since epoch: 8363.741. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1670. time since epoch: 8412.998. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1680. time since epoch: 8461.349. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1690. time since epoch: 8514.585. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1700. time since epoch: 8565.545. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1710. time since epoch: 8615.448. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1720. time since epoch: 8663.356. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1730. time since epoch: 8711.176. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1740. time since epoch: 8759.295. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1750. time since epoch: 8807.375. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1760. time since epoch: 8855.729. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1770. time since epoch: 8905.062. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1780. time since epoch: 8953.675. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1790. time since epoch: 9002.062. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1800. time since epoch: 9052.215. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1810. time since epoch: 9103.818. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1820. time since epoch: 9154.162. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1830. time since epoch: 9203.551. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1840. time since epoch: 9253.118. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1850. time since epoch: 9301.988. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1860. time since epoch: 9350.756. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1870. time since epoch: 9400.998. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1880. time since epoch: 9451.203. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1890. time since epoch: 9508.440. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1900. time since epoch: 9567.742. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1910. time since epoch: 9627.015. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1920. time since epoch: 9680.861. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1930. time since epoch: 9735.790. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1940. time since epoch: 9790.254. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1950. time since epoch: 9845.407. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1960. time since epoch: 9897.576. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1970. time since epoch: 9953.533. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1980. time since epoch: 10005.029. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 1990. time since epoch: 10057.929. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 2000. time since epoch: 10115.838. Train acc: 0.906. Train Loss: 0.233\n",
      "Step 2010. time since epoch: 10169.607. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2020. time since epoch: 10222.965. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2030. time since epoch: 10274.433. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2040. time since epoch: 10324.931. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2050. time since epoch: 10376.723. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2060. time since epoch: 10425.904. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2070. time since epoch: 10474.542. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2080. time since epoch: 10523.272. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2090. time since epoch: 10572.078. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2100. time since epoch: 10620.307. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2110. time since epoch: 10668.578. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2120. time since epoch: 10718.539. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2130. time since epoch: 10767.944. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2140. time since epoch: 10816.321. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2150. time since epoch: 10863.938. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2160. time since epoch: 10911.706. Train acc: 0.907. Train Loss: 0.232\n",
      "Step 2170. time since epoch: 10961.448. Train acc: 0.907. Train Loss: 0.232\n",
      "Step 2180. time since epoch: 11010.383. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2190. time since epoch: 11058.492. Train acc: 0.907. Train Loss: 0.232\n",
      "Step 2200. time since epoch: 11108.352. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2210. time since epoch: 11161.435. Train acc: 0.907. Train Loss: 0.232\n",
      "Step 2220. time since epoch: 11210.573. Train acc: 0.907. Train Loss: 0.231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2230. time since epoch: 11258.569. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2240. time since epoch: 11306.762. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2250. time since epoch: 11354.603. Train acc: 0.907. Train Loss: 0.232\n",
      "Step 2260. time since epoch: 11402.111. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2270. time since epoch: 11453.023. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2280. time since epoch: 11505.660. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2290. time since epoch: 11557.451. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2300. time since epoch: 11668.398. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2310. time since epoch: 11720.964. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2320. time since epoch: 11772.825. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2330. time since epoch: 11827.746. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2340. time since epoch: 11882.751. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2350. time since epoch: 11934.013. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2360. time since epoch: 11983.769. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2370. time since epoch: 12034.460. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2380. time since epoch: 12088.924. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2390. time since epoch: 12142.554. Train acc: 0.906. Train Loss: 0.232\n",
      "Step 2400. time since epoch: 12196.350. Train acc: 0.906. Train Loss: 0.231\n",
      "Step 2410. time since epoch: 12248.958. Train acc: 0.906. Train Loss: 0.231\n",
      "Step 2420. time since epoch: 12303.412. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2430. time since epoch: 12355.163. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2440. time since epoch: 12409.747. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2450. time since epoch: 12463.322. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2460. time since epoch: 12516.506. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2470. time since epoch: 12568.684. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2480. time since epoch: 12621.551. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2490. time since epoch: 12672.806. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2500. time since epoch: 12721.776. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2510. time since epoch: 12772.735. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2520. time since epoch: 12823.712. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2530. time since epoch: 12874.667. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2540. time since epoch: 12925.286. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2550. time since epoch: 12975.008. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2560. time since epoch: 13024.864. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2570. time since epoch: 13078.979. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2580. time since epoch: 13132.876. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2590. time since epoch: 13185.841. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2600. time since epoch: 13239.204. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2610. time since epoch: 13290.727. Train acc: 0.907. Train Loss: 0.231\n",
      "Step 2620. time since epoch: 13343.369. Train acc: 0.907. Train Loss: 0.230\n",
      "Step 2630. time since epoch: 13394.285. Train acc: 0.907. Train Loss: 0.230\n",
      "Step 2640. time since epoch: 13448.914. Train acc: 0.907. Train Loss: 0.230\n",
      "Step 2650. time since epoch: 13500.949. Train acc: 0.907. Train Loss: 0.230\n",
      "Step 2660. time since epoch: 13551.068. Train acc: 0.907. Train Loss: 0.230\n",
      "Step 2670. time since epoch: 13600.931. Train acc: 0.907. Train Loss: 0.230\n",
      "Step 2680. time since epoch: 13649.927. Train acc: 0.907. Train Loss: 0.230\n",
      "Step 2690. time since epoch: 13700.883. Train acc: 0.907. Train Loss: 0.230\n",
      "Step 2700. time since epoch: 13751.875. Train acc: 0.907. Train Loss: 0.230\n",
      "Step 2710. time since epoch: 13801.366. Train acc: 0.907. Train Loss: 0.230\n",
      "Step 2720. time since epoch: 13855.785. Train acc: 0.907. Train Loss: 0.230\n",
      "--------------------\n",
      "epoch 7, loss 0.2298, train acc 0.907, test acc 0.860, time 15011.3 sec\n",
      "Step 0. time since epoch: 7.562. Train acc: 0.938. Train Loss: 0.166\n",
      "Step 10. time since epoch: 66.917. Train acc: 0.906. Train Loss: 0.230\n",
      "Step 20. time since epoch: 121.991. Train acc: 0.906. Train Loss: 0.226\n",
      "Step 30. time since epoch: 175.647. Train acc: 0.906. Train Loss: 0.225\n",
      "Step 40. time since epoch: 225.906. Train acc: 0.903. Train Loss: 0.229\n",
      "Step 50. time since epoch: 278.593. Train acc: 0.906. Train Loss: 0.224\n",
      "Step 60. time since epoch: 332.913. Train acc: 0.906. Train Loss: 0.227\n",
      "Step 70. time since epoch: 396.944. Train acc: 0.908. Train Loss: 0.225\n",
      "Step 80. time since epoch: 461.130. Train acc: 0.909. Train Loss: 0.222\n",
      "Step 90. time since epoch: 525.733. Train acc: 0.910. Train Loss: 0.221\n",
      "Step 100. time since epoch: 589.671. Train acc: 0.911. Train Loss: 0.219\n",
      "Step 110. time since epoch: 653.271. Train acc: 0.912. Train Loss: 0.218\n",
      "Step 120. time since epoch: 717.126. Train acc: 0.912. Train Loss: 0.217\n",
      "Step 130. time since epoch: 780.318. Train acc: 0.912. Train Loss: 0.216\n",
      "Step 140. time since epoch: 844.155. Train acc: 0.912. Train Loss: 0.217\n",
      "Step 150. time since epoch: 907.862. Train acc: 0.912. Train Loss: 0.217\n",
      "Step 160. time since epoch: 971.026. Train acc: 0.912. Train Loss: 0.216\n",
      "Step 170. time since epoch: 1019.894. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 180. time since epoch: 1068.541. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 190. time since epoch: 1117.123. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 200. time since epoch: 1166.486. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 210. time since epoch: 1215.048. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 220. time since epoch: 1263.569. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 230. time since epoch: 1312.579. Train acc: 0.913. Train Loss: 0.217\n",
      "Step 240. time since epoch: 1361.632. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 250. time since epoch: 1410.678. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 260. time since epoch: 1459.659. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 270. time since epoch: 1508.326. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 280. time since epoch: 1557.203. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 290. time since epoch: 1606.029. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 300. time since epoch: 1654.775. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 310. time since epoch: 1703.861. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 320. time since epoch: 1753.393. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 330. time since epoch: 1802.164. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 340. time since epoch: 1852.342. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 350. time since epoch: 1902.315. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 360. time since epoch: 1952.973. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 370. time since epoch: 2004.095. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 380. time since epoch: 2054.642. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 390. time since epoch: 2104.992. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 400. time since epoch: 2155.566. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 410. time since epoch: 2206.846. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 420. time since epoch: 2257.595. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 430. time since epoch: 2308.566. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 440. time since epoch: 2359.577. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 450. time since epoch: 2410.571. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 460. time since epoch: 2462.417. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 470. time since epoch: 2512.293. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 480. time since epoch: 2562.052. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 490. time since epoch: 2612.379. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 500. time since epoch: 2663.326. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 510. time since epoch: 2712.644. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 520. time since epoch: 2762.009. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 530. time since epoch: 2812.737. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 540. time since epoch: 2863.241. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 550. time since epoch: 2913.984. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 560. time since epoch: 2964.469. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 570. time since epoch: 3015.023. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 580. time since epoch: 3065.498. Train acc: 0.913. Train Loss: 0.215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 590. time since epoch: 3116.135. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 600. time since epoch: 3166.498. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 610. time since epoch: 3217.156. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 620. time since epoch: 3267.518. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 630. time since epoch: 3317.492. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 640. time since epoch: 3367.683. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 650. time since epoch: 3417.551. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 660. time since epoch: 3467.811. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 670. time since epoch: 3517.988. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 680. time since epoch: 3568.860. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 690. time since epoch: 3619.548. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 700. time since epoch: 3670.081. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 710. time since epoch: 3720.507. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 720. time since epoch: 3770.639. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 730. time since epoch: 3820.779. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 740. time since epoch: 3869.226. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 750. time since epoch: 3918.136. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 760. time since epoch: 3967.335. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 770. time since epoch: 4017.050. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 780. time since epoch: 4067.556. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 790. time since epoch: 4116.699. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 800. time since epoch: 4165.991. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 810. time since epoch: 4215.948. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 820. time since epoch: 4265.510. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 830. time since epoch: 4315.397. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 840. time since epoch: 4365.484. Train acc: 0.913. Train Loss: 0.216\n",
      "Step 850. time since epoch: 4415.696. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 860. time since epoch: 4466.547. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 870. time since epoch: 4515.108. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 880. time since epoch: 4563.919. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 890. time since epoch: 4613.063. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 900. time since epoch: 4662.956. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 910. time since epoch: 4712.864. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 920. time since epoch: 4761.475. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 930. time since epoch: 4811.452. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 940. time since epoch: 4860.802. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 950. time since epoch: 4908.983. Train acc: 0.914. Train Loss: 0.214\n",
      "Step 960. time since epoch: 4957.503. Train acc: 0.914. Train Loss: 0.214\n",
      "Step 970. time since epoch: 5006.174. Train acc: 0.914. Train Loss: 0.214\n",
      "Step 980. time since epoch: 5055.043. Train acc: 0.914. Train Loss: 0.214\n",
      "Step 990. time since epoch: 5104.502. Train acc: 0.914. Train Loss: 0.214\n",
      "Step 1000. time since epoch: 5154.147. Train acc: 0.914. Train Loss: 0.214\n",
      "Step 1010. time since epoch: 5204.051. Train acc: 0.914. Train Loss: 0.214\n",
      "Step 1020. time since epoch: 5253.588. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1030. time since epoch: 5304.184. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1040. time since epoch: 5353.013. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1050. time since epoch: 5401.255. Train acc: 0.914. Train Loss: 0.214\n",
      "Step 1060. time since epoch: 5449.934. Train acc: 0.914. Train Loss: 0.214\n",
      "Step 1070. time since epoch: 5499.830. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1080. time since epoch: 5549.575. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1090. time since epoch: 5598.581. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1100. time since epoch: 5647.502. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1110. time since epoch: 5695.727. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1120. time since epoch: 5744.109. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1130. time since epoch: 5792.444. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1140. time since epoch: 5840.479. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1150. time since epoch: 5888.663. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1160. time since epoch: 5937.285. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1170. time since epoch: 5985.783. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1180. time since epoch: 6034.152. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1190. time since epoch: 6082.912. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1200. time since epoch: 6131.994. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1210. time since epoch: 6181.243. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1220. time since epoch: 6231.053. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1230. time since epoch: 6280.594. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1240. time since epoch: 6329.566. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1250. time since epoch: 6379.519. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1260. time since epoch: 6429.310. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1270. time since epoch: 6478.947. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1280. time since epoch: 6527.917. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1290. time since epoch: 6576.800. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1300. time since epoch: 6625.310. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1310. time since epoch: 6674.059. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1320. time since epoch: 6722.718. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1330. time since epoch: 6771.058. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1340. time since epoch: 6820.524. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1350. time since epoch: 6870.619. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1360. time since epoch: 6920.626. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1370. time since epoch: 6973.147. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1380. time since epoch: 7023.739. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1390. time since epoch: 7073.973. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1400. time since epoch: 7123.436. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1410. time since epoch: 7174.912. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1420. time since epoch: 7226.699. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1430. time since epoch: 7280.580. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1440. time since epoch: 7335.189. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1450. time since epoch: 7387.579. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1460. time since epoch: 7439.468. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1470. time since epoch: 7491.974. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1480. time since epoch: 7544.033. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1490. time since epoch: 7596.150. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1500. time since epoch: 7648.628. Train acc: 0.913. Train Loss: 0.215\n",
      "Step 1510. time since epoch: 7700.444. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1520. time since epoch: 7751.536. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1530. time since epoch: 7802.852. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1540. time since epoch: 7853.841. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1550. time since epoch: 7906.158. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1560. time since epoch: 7958.913. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1570. time since epoch: 8008.998. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1580. time since epoch: 8060.692. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1590. time since epoch: 8111.764. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1600. time since epoch: 8163.022. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1610. time since epoch: 8213.657. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1620. time since epoch: 8264.899. Train acc: 0.913. Train Loss: 0.214\n",
      "Step 1630. time since epoch: 8316.290. Train acc: 0.914. Train Loss: 0.214\n",
      "Step 1640. time since epoch: 8367.836. Train acc: 0.914. Train Loss: 0.214\n",
      "Step 1650. time since epoch: 8418.875. Train acc: 0.914. Train Loss: 0.214\n",
      "Step 1660. time since epoch: 8466.916. Train acc: 0.914. Train Loss: 0.214\n",
      "Step 1670. time since epoch: 8515.071. Train acc: 0.914. Train Loss: 0.214\n",
      "Step 1680. time since epoch: 8563.257. Train acc: 0.914. Train Loss: 0.214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1690. time since epoch: 8611.486. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1700. time since epoch: 8659.591. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1710. time since epoch: 8708.946. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1720. time since epoch: 8757.122. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1730. time since epoch: 8804.722. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1740. time since epoch: 8852.595. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1750. time since epoch: 8900.578. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1760. time since epoch: 8948.793. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1770. time since epoch: 8996.909. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1780. time since epoch: 9044.845. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1790. time since epoch: 9092.582. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1800. time since epoch: 9140.739. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1810. time since epoch: 9188.670. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1820. time since epoch: 9236.486. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1830. time since epoch: 9285.076. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1840. time since epoch: 9333.172. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1850. time since epoch: 9381.225. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1860. time since epoch: 9428.892. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1870. time since epoch: 9476.558. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1880. time since epoch: 9524.517. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1890. time since epoch: 9573.015. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1900. time since epoch: 9620.884. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1910. time since epoch: 9669.233. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1920. time since epoch: 9717.261. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1930. time since epoch: 9765.267. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1940. time since epoch: 9813.192. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1950. time since epoch: 9861.437. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1960. time since epoch: 9909.097. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1970. time since epoch: 9956.920. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1980. time since epoch: 10005.062. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 1990. time since epoch: 10053.080. Train acc: 0.914. Train Loss: 0.213\n",
      "Step 2000. time since epoch: 10100.841. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2010. time since epoch: 10148.760. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2020. time since epoch: 10196.777. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2030. time since epoch: 10244.879. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2040. time since epoch: 10292.692. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2050. time since epoch: 10340.418. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2060. time since epoch: 10388.650. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2070. time since epoch: 10436.987. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2080. time since epoch: 10486.039. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2090. time since epoch: 10534.192. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2100. time since epoch: 10582.644. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2110. time since epoch: 10630.812. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2120. time since epoch: 10678.873. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2130. time since epoch: 10726.811. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2140. time since epoch: 10774.968. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2150. time since epoch: 10823.061. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2160. time since epoch: 10871.232. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2170. time since epoch: 10919.437. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2180. time since epoch: 10967.837. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2190. time since epoch: 11016.547. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2200. time since epoch: 11065.019. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2210. time since epoch: 11113.306. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2220. time since epoch: 11161.494. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2230. time since epoch: 11209.668. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2240. time since epoch: 11258.340. Train acc: 0.914. Train Loss: 0.211\n",
      "Step 2250. time since epoch: 11308.947. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2260. time since epoch: 11358.376. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2270. time since epoch: 11408.937. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2280. time since epoch: 11458.114. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2290. time since epoch: 11507.815. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2300. time since epoch: 11558.299. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2310. time since epoch: 11608.866. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2320. time since epoch: 11659.197. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2330. time since epoch: 11709.498. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2340. time since epoch: 11760.629. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2350. time since epoch: 11809.730. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2360. time since epoch: 11857.893. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2370. time since epoch: 11906.005. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2380. time since epoch: 11953.822. Train acc: 0.914. Train Loss: 0.212\n",
      "Step 2390. time since epoch: 12002.073. Train acc: 0.914. Train Loss: 0.211\n",
      "Step 2400. time since epoch: 12050.797. Train acc: 0.914. Train Loss: 0.211\n",
      "Step 2410. time since epoch: 12098.876. Train acc: 0.914. Train Loss: 0.211\n",
      "Step 2420. time since epoch: 12147.104. Train acc: 0.914. Train Loss: 0.211\n",
      "Step 2430. time since epoch: 12195.345. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2440. time since epoch: 12243.703. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2450. time since epoch: 12293.388. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2460. time since epoch: 12341.704. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2470. time since epoch: 12390.432. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2480. time since epoch: 12438.762. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2490. time since epoch: 12487.920. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2500. time since epoch: 12537.330. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2510. time since epoch: 12587.346. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2520. time since epoch: 12637.346. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2530. time since epoch: 12687.220. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2540. time since epoch: 12737.261. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2550. time since epoch: 12787.436. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2560. time since epoch: 12837.449. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2570. time since epoch: 12887.241. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2580. time since epoch: 12936.963. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2590. time since epoch: 12986.881. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2600. time since epoch: 13037.081. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2610. time since epoch: 13088.086. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2620. time since epoch: 13137.403. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2630. time since epoch: 13185.436. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 2640. time since epoch: 13233.433. Train acc: 0.915. Train Loss: 0.210\n",
      "Step 2650. time since epoch: 13283.121. Train acc: 0.915. Train Loss: 0.210\n",
      "Step 2660. time since epoch: 13331.290. Train acc: 0.915. Train Loss: 0.210\n",
      "Step 2670. time since epoch: 13380.629. Train acc: 0.915. Train Loss: 0.210\n",
      "Step 2680. time since epoch: 13430.429. Train acc: 0.915. Train Loss: 0.210\n",
      "Step 2690. time since epoch: 13480.102. Train acc: 0.915. Train Loss: 0.210\n",
      "Step 2700. time since epoch: 13530.153. Train acc: 0.915. Train Loss: 0.210\n",
      "Step 2710. time since epoch: 13579.526. Train acc: 0.915. Train Loss: 0.210\n",
      "Step 2720. time since epoch: 13629.154. Train acc: 0.915. Train Loss: 0.210\n",
      "--------------------\n",
      "epoch 8, loss 0.2100, train acc 0.915, test acc 0.853, time 14602.1 sec\n",
      "Step 0. time since epoch: 5.097. Train acc: 0.941. Train Loss: 0.158\n",
      "Step 10. time since epoch: 53.257. Train acc: 0.915. Train Loss: 0.212\n",
      "Step 20. time since epoch: 101.558. Train acc: 0.916. Train Loss: 0.210\n",
      "Step 30. time since epoch: 151.348. Train acc: 0.917. Train Loss: 0.206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40. time since epoch: 201.574. Train acc: 0.915. Train Loss: 0.211\n",
      "Step 50. time since epoch: 251.673. Train acc: 0.917. Train Loss: 0.206\n",
      "Step 60. time since epoch: 303.219. Train acc: 0.917. Train Loss: 0.206\n",
      "Step 70. time since epoch: 353.444. Train acc: 0.919. Train Loss: 0.202\n",
      "Step 80. time since epoch: 403.347. Train acc: 0.920. Train Loss: 0.199\n",
      "Step 90. time since epoch: 452.774. Train acc: 0.921. Train Loss: 0.200\n",
      "Step 100. time since epoch: 502.741. Train acc: 0.922. Train Loss: 0.198\n",
      "Step 110. time since epoch: 553.045. Train acc: 0.922. Train Loss: 0.196\n",
      "Step 120. time since epoch: 602.801. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 130. time since epoch: 653.295. Train acc: 0.923. Train Loss: 0.194\n",
      "Step 140. time since epoch: 703.224. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 150. time since epoch: 753.874. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 160. time since epoch: 803.930. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 170. time since epoch: 853.501. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 180. time since epoch: 903.555. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 190. time since epoch: 953.039. Train acc: 0.922. Train Loss: 0.196\n",
      "Step 200. time since epoch: 1002.838. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 210. time since epoch: 1053.385. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 220. time since epoch: 1103.731. Train acc: 0.922. Train Loss: 0.196\n",
      "Step 230. time since epoch: 1153.536. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 240. time since epoch: 1203.509. Train acc: 0.921. Train Loss: 0.197\n",
      "Step 250. time since epoch: 1253.450. Train acc: 0.922. Train Loss: 0.197\n",
      "Step 260. time since epoch: 1304.050. Train acc: 0.922. Train Loss: 0.197\n",
      "Step 270. time since epoch: 1354.919. Train acc: 0.922. Train Loss: 0.197\n",
      "Step 280. time since epoch: 1404.869. Train acc: 0.922. Train Loss: 0.196\n",
      "Step 290. time since epoch: 1455.091. Train acc: 0.921. Train Loss: 0.197\n",
      "Step 300. time since epoch: 1505.121. Train acc: 0.921. Train Loss: 0.197\n",
      "Step 310. time since epoch: 1555.200. Train acc: 0.921. Train Loss: 0.197\n",
      "Step 320. time since epoch: 1605.036. Train acc: 0.921. Train Loss: 0.197\n",
      "Step 330. time since epoch: 1654.612. Train acc: 0.921. Train Loss: 0.197\n",
      "Step 340. time since epoch: 1705.100. Train acc: 0.921. Train Loss: 0.197\n",
      "Step 350. time since epoch: 1754.241. Train acc: 0.921. Train Loss: 0.198\n",
      "Step 360. time since epoch: 1804.540. Train acc: 0.921. Train Loss: 0.199\n",
      "Step 370. time since epoch: 1854.107. Train acc: 0.921. Train Loss: 0.199\n",
      "Step 380. time since epoch: 1903.552. Train acc: 0.921. Train Loss: 0.198\n",
      "Step 390. time since epoch: 1954.102. Train acc: 0.921. Train Loss: 0.197\n",
      "Step 400. time since epoch: 2003.890. Train acc: 0.921. Train Loss: 0.198\n",
      "Step 410. time since epoch: 2054.194. Train acc: 0.921. Train Loss: 0.198\n",
      "Step 420. time since epoch: 2103.758. Train acc: 0.921. Train Loss: 0.198\n",
      "Step 430. time since epoch: 2153.320. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 440. time since epoch: 2202.788. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 450. time since epoch: 2251.927. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 460. time since epoch: 2300.854. Train acc: 0.920. Train Loss: 0.199\n",
      "Step 470. time since epoch: 2350.487. Train acc: 0.920. Train Loss: 0.199\n",
      "Step 480. time since epoch: 2400.283. Train acc: 0.920. Train Loss: 0.199\n",
      "Step 490. time since epoch: 2450.245. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 500. time since epoch: 2500.002. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 510. time since epoch: 2550.400. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 520. time since epoch: 2601.050. Train acc: 0.920. Train Loss: 0.199\n",
      "Step 530. time since epoch: 2651.438. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 540. time since epoch: 2701.047. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 550. time since epoch: 2750.258. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 560. time since epoch: 2800.317. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 570. time since epoch: 2850.622. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 580. time since epoch: 2900.621. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 590. time since epoch: 2950.168. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 600. time since epoch: 3000.056. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 610. time since epoch: 3049.380. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 620. time since epoch: 3098.786. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 630. time since epoch: 3150.211. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 640. time since epoch: 3200.148. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 650. time since epoch: 3250.508. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 660. time since epoch: 3299.991. Train acc: 0.921. Train Loss: 0.198\n",
      "Step 670. time since epoch: 3350.162. Train acc: 0.921. Train Loss: 0.198\n",
      "Step 680. time since epoch: 3400.030. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 690. time since epoch: 3450.328. Train acc: 0.920. Train Loss: 0.197\n",
      "Step 700. time since epoch: 3499.658. Train acc: 0.920. Train Loss: 0.197\n",
      "Step 710. time since epoch: 3549.535. Train acc: 0.920. Train Loss: 0.197\n",
      "Step 720. time since epoch: 3599.799. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 730. time since epoch: 3648.948. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 740. time since epoch: 3698.789. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 750. time since epoch: 3748.683. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 760. time since epoch: 3797.785. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 770. time since epoch: 3847.271. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 780. time since epoch: 3898.251. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 790. time since epoch: 3948.471. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 800. time since epoch: 3998.676. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 810. time since epoch: 4049.102. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 820. time since epoch: 4098.736. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 830. time since epoch: 4148.787. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 840. time since epoch: 4198.787. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 850. time since epoch: 4248.987. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 860. time since epoch: 4298.545. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 870. time since epoch: 4348.696. Train acc: 0.920. Train Loss: 0.198\n",
      "Step 880. time since epoch: 4398.257. Train acc: 0.920. Train Loss: 0.197\n",
      "Step 890. time since epoch: 4447.814. Train acc: 0.920. Train Loss: 0.197\n",
      "Step 900. time since epoch: 4497.687. Train acc: 0.921. Train Loss: 0.197\n",
      "Step 910. time since epoch: 4547.489. Train acc: 0.921. Train Loss: 0.197\n",
      "Step 920. time since epoch: 4597.108. Train acc: 0.921. Train Loss: 0.197\n",
      "Step 930. time since epoch: 4647.163. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 940. time since epoch: 4697.022. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 950. time since epoch: 4747.082. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 960. time since epoch: 4796.750. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 970. time since epoch: 4846.557. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 980. time since epoch: 4896.136. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 990. time since epoch: 4946.505. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1000. time since epoch: 4995.228. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1010. time since epoch: 5044.421. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1020. time since epoch: 5094.081. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1030. time since epoch: 5144.083. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1040. time since epoch: 5195.093. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1050. time since epoch: 5245.304. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1060. time since epoch: 5295.604. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1070. time since epoch: 5344.628. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1080. time since epoch: 5392.813. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1090. time since epoch: 5441.665. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1100. time since epoch: 5490.890. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1110. time since epoch: 5539.943. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1120. time since epoch: 5588.415. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1130. time since epoch: 5637.243. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1140. time since epoch: 5686.468. Train acc: 0.921. Train Loss: 0.196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1150. time since epoch: 5735.664. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1160. time since epoch: 5785.118. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1170. time since epoch: 5834.873. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1180. time since epoch: 5882.600. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1190. time since epoch: 5930.422. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1200. time since epoch: 5978.539. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1210. time since epoch: 6026.853. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1220. time since epoch: 6075.947. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1230. time since epoch: 6124.852. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1240. time since epoch: 6175.815. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1250. time since epoch: 6224.668. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1260. time since epoch: 6273.257. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1270. time since epoch: 6321.893. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1280. time since epoch: 6370.367. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1290. time since epoch: 6418.628. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1300. time since epoch: 6467.331. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1310. time since epoch: 6515.844. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1320. time since epoch: 6564.367. Train acc: 0.921. Train Loss: 0.197\n",
      "Step 1330. time since epoch: 6612.975. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1340. time since epoch: 6661.758. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1350. time since epoch: 6711.058. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1360. time since epoch: 6760.133. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1370. time since epoch: 6808.741. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1380. time since epoch: 6857.216. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1390. time since epoch: 6906.228. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1400. time since epoch: 6956.056. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1410. time since epoch: 7005.667. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1420. time since epoch: 7054.418. Train acc: 0.921. Train Loss: 0.197\n",
      "Step 1430. time since epoch: 7103.041. Train acc: 0.921. Train Loss: 0.197\n",
      "Step 1440. time since epoch: 7152.215. Train acc: 0.921. Train Loss: 0.197\n",
      "Step 1450. time since epoch: 7201.116. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1460. time since epoch: 7250.731. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1470. time since epoch: 7300.575. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1480. time since epoch: 7350.505. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1490. time since epoch: 7400.369. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1500. time since epoch: 7449.023. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1510. time since epoch: 7497.364. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1520. time since epoch: 7546.533. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1530. time since epoch: 7595.599. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1540. time since epoch: 7644.785. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1550. time since epoch: 7693.440. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1560. time since epoch: 7742.088. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1570. time since epoch: 7791.169. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1580. time since epoch: 7840.660. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1590. time since epoch: 7890.136. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1600. time since epoch: 7939.567. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1610. time since epoch: 7988.822. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1620. time since epoch: 8038.037. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1630. time since epoch: 8087.106. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1640. time since epoch: 8135.999. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1650. time since epoch: 8185.030. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1660. time since epoch: 8233.629. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1670. time since epoch: 8282.540. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1680. time since epoch: 8331.691. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1690. time since epoch: 8381.085. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1700. time since epoch: 8430.470. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1710. time since epoch: 8479.268. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1720. time since epoch: 8527.926. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1730. time since epoch: 8576.252. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1740. time since epoch: 8624.598. Train acc: 0.921. Train Loss: 0.195\n",
      "Step 1750. time since epoch: 8672.703. Train acc: 0.921. Train Loss: 0.195\n",
      "Step 1760. time since epoch: 8721.133. Train acc: 0.921. Train Loss: 0.195\n",
      "Step 1770. time since epoch: 8770.657. Train acc: 0.921. Train Loss: 0.195\n",
      "Step 1780. time since epoch: 8820.036. Train acc: 0.921. Train Loss: 0.195\n",
      "Step 1790. time since epoch: 8869.912. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1800. time since epoch: 8919.710. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1810. time since epoch: 8969.091. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1820. time since epoch: 9018.439. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1830. time since epoch: 9067.637. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1840. time since epoch: 9117.111. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1850. time since epoch: 9166.943. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1860. time since epoch: 9215.573. Train acc: 0.921. Train Loss: 0.196\n",
      "Step 1870. time since epoch: 9264.180. Train acc: 0.921. Train Loss: 0.195\n",
      "Step 1880. time since epoch: 9312.857. Train acc: 0.921. Train Loss: 0.195\n",
      "Step 1890. time since epoch: 9361.518. Train acc: 0.921. Train Loss: 0.195\n",
      "Step 1900. time since epoch: 9410.343. Train acc: 0.921. Train Loss: 0.195\n",
      "Step 1910. time since epoch: 9459.668. Train acc: 0.921. Train Loss: 0.195\n",
      "Step 1920. time since epoch: 9508.214. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 1930. time since epoch: 9556.696. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 1940. time since epoch: 9605.874. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 1950. time since epoch: 9655.925. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 1960. time since epoch: 9705.386. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 1970. time since epoch: 9755.303. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 1980. time since epoch: 9804.326. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 1990. time since epoch: 9853.949. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 2000. time since epoch: 9903.271. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 2010. time since epoch: 9951.619. Train acc: 0.922. Train Loss: 0.195\n",
      "Step 2020. time since epoch: 10000.273. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2030. time since epoch: 10048.945. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2040. time since epoch: 10098.525. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2050. time since epoch: 10147.754. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2060. time since epoch: 10197.001. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2070. time since epoch: 10245.592. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2080. time since epoch: 10293.908. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2090. time since epoch: 10342.603. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2100. time since epoch: 10391.674. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2110. time since epoch: 10440.812. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2120. time since epoch: 10490.249. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2130. time since epoch: 10538.767. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2140. time since epoch: 10586.991. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2150. time since epoch: 10635.559. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2160. time since epoch: 10684.359. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2170. time since epoch: 10732.983. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2180. time since epoch: 10782.605. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2190. time since epoch: 10832.029. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2200. time since epoch: 10881.568. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2210. time since epoch: 10931.067. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2220. time since epoch: 10980.648. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2230. time since epoch: 11030.282. Train acc: 0.922. Train Loss: 0.193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2240. time since epoch: 11079.823. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2250. time since epoch: 11129.623. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2260. time since epoch: 11179.067. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2270. time since epoch: 11227.529. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2280. time since epoch: 11276.472. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2290. time since epoch: 11324.348. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2300. time since epoch: 11372.138. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2310. time since epoch: 11419.287. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2320. time since epoch: 11466.901. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2330. time since epoch: 11515.043. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2340. time since epoch: 11564.621. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2350. time since epoch: 11614.105. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2360. time since epoch: 11663.229. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2370. time since epoch: 11712.587. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2380. time since epoch: 11762.125. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2390. time since epoch: 11811.609. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2400. time since epoch: 11861.088. Train acc: 0.922. Train Loss: 0.194\n",
      "Step 2410. time since epoch: 11909.908. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2420. time since epoch: 11959.231. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2430. time since epoch: 12008.853. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2440. time since epoch: 12058.217. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2450. time since epoch: 12106.868. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2460. time since epoch: 12156.497. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2470. time since epoch: 12205.980. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2480. time since epoch: 12255.136. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2490. time since epoch: 12304.555. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2500. time since epoch: 12353.768. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2510. time since epoch: 12402.814. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2520. time since epoch: 12451.882. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2530. time since epoch: 12500.280. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2540. time since epoch: 12548.417. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2550. time since epoch: 12596.886. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2560. time since epoch: 12645.432. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2570. time since epoch: 12693.999. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2580. time since epoch: 12742.452. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2590. time since epoch: 12791.407. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2600. time since epoch: 12839.107. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2610. time since epoch: 12887.024. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2620. time since epoch: 12934.921. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2630. time since epoch: 12982.834. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2640. time since epoch: 13030.968. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2650. time since epoch: 13080.296. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2660. time since epoch: 13128.052. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2670. time since epoch: 13176.011. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2680. time since epoch: 13224.230. Train acc: 0.922. Train Loss: 0.193\n",
      "Step 2690. time since epoch: 13273.247. Train acc: 0.922. Train Loss: 0.192\n",
      "Step 2700. time since epoch: 13322.590. Train acc: 0.922. Train Loss: 0.192\n",
      "Step 2710. time since epoch: 13371.199. Train acc: 0.923. Train Loss: 0.192\n",
      "Step 2720. time since epoch: 13419.701. Train acc: 0.923. Train Loss: 0.192\n",
      "--------------------\n",
      "epoch 9, loss 0.1923, train acc 0.923, test acc 0.854, time 14351.5 sec\n",
      "Step 0. time since epoch: 4.850. Train acc: 0.945. Train Loss: 0.124\n",
      "Step 10. time since epoch: 54.718. Train acc: 0.926. Train Loss: 0.182\n",
      "Step 20. time since epoch: 104.042. Train acc: 0.926. Train Loss: 0.183\n",
      "Step 30. time since epoch: 152.722. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 40. time since epoch: 201.540. Train acc: 0.925. Train Loss: 0.185\n",
      "Step 50. time since epoch: 250.030. Train acc: 0.926. Train Loss: 0.182\n",
      "Step 60. time since epoch: 298.349. Train acc: 0.926. Train Loss: 0.183\n",
      "Step 70. time since epoch: 346.727. Train acc: 0.926. Train Loss: 0.182\n",
      "Step 80. time since epoch: 395.201. Train acc: 0.928. Train Loss: 0.178\n",
      "Step 90. time since epoch: 444.384. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 100. time since epoch: 493.614. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 110. time since epoch: 542.544. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 120. time since epoch: 590.779. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 130. time since epoch: 638.900. Train acc: 0.929. Train Loss: 0.175\n",
      "Step 140. time since epoch: 687.822. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 150. time since epoch: 737.192. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 160. time since epoch: 786.664. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 170. time since epoch: 836.361. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 180. time since epoch: 885.657. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 190. time since epoch: 935.209. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 200. time since epoch: 983.801. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 210. time since epoch: 1032.545. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 220. time since epoch: 1081.171. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 230. time since epoch: 1129.922. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 240. time since epoch: 1178.344. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 250. time since epoch: 1226.848. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 260. time since epoch: 1275.334. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 270. time since epoch: 1323.743. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 280. time since epoch: 1372.289. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 290. time since epoch: 1421.177. Train acc: 0.929. Train Loss: 0.179\n",
      "Step 300. time since epoch: 1469.624. Train acc: 0.929. Train Loss: 0.179\n",
      "Step 310. time since epoch: 1517.808. Train acc: 0.929. Train Loss: 0.179\n",
      "Step 320. time since epoch: 1565.568. Train acc: 0.929. Train Loss: 0.179\n",
      "Step 330. time since epoch: 1614.074. Train acc: 0.929. Train Loss: 0.179\n",
      "Step 340. time since epoch: 1662.481. Train acc: 0.929. Train Loss: 0.179\n",
      "Step 350. time since epoch: 1711.056. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 360. time since epoch: 1759.374. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 370. time since epoch: 1807.631. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 380. time since epoch: 1856.219. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 390. time since epoch: 1904.734. Train acc: 0.928. Train Loss: 0.179\n",
      "Step 400. time since epoch: 1953.367. Train acc: 0.928. Train Loss: 0.179\n",
      "Step 410. time since epoch: 2001.974. Train acc: 0.928. Train Loss: 0.179\n",
      "Step 420. time since epoch: 2050.492. Train acc: 0.929. Train Loss: 0.179\n",
      "Step 430. time since epoch: 2098.959. Train acc: 0.928. Train Loss: 0.179\n",
      "Step 440. time since epoch: 2147.625. Train acc: 0.928. Train Loss: 0.179\n",
      "Step 450. time since epoch: 2196.253. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 460. time since epoch: 2244.624. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 470. time since epoch: 2293.396. Train acc: 0.928. Train Loss: 0.181\n",
      "Step 480. time since epoch: 2342.219. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 490. time since epoch: 2390.434. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 500. time since epoch: 2438.954. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 510. time since epoch: 2487.740. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 520. time since epoch: 2537.247. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 530. time since epoch: 2586.540. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 540. time since epoch: 2635.171. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 550. time since epoch: 2683.737. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 560. time since epoch: 2732.280. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 570. time since epoch: 2780.284. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 580. time since epoch: 2828.043. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 590. time since epoch: 2875.828. Train acc: 0.928. Train Loss: 0.180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 600. time since epoch: 2924.153. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 610. time since epoch: 2972.131. Train acc: 0.928. Train Loss: 0.180\n",
      "Step 620. time since epoch: 3019.900. Train acc: 0.928. Train Loss: 0.179\n",
      "Step 630. time since epoch: 3067.646. Train acc: 0.928. Train Loss: 0.179\n",
      "Step 640. time since epoch: 3116.060. Train acc: 0.928. Train Loss: 0.179\n",
      "Step 650. time since epoch: 3164.198. Train acc: 0.928. Train Loss: 0.179\n",
      "Step 660. time since epoch: 3212.850. Train acc: 0.929. Train Loss: 0.179\n",
      "Step 670. time since epoch: 3262.026. Train acc: 0.928. Train Loss: 0.178\n",
      "Step 680. time since epoch: 3310.283. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 690. time since epoch: 3358.606. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 700. time since epoch: 3406.631. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 710. time since epoch: 3455.349. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 720. time since epoch: 3504.224. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 730. time since epoch: 3552.931. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 740. time since epoch: 3601.250. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 750. time since epoch: 3650.031. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 760. time since epoch: 3699.017. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 770. time since epoch: 3747.693. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 780. time since epoch: 3796.343. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 790. time since epoch: 3845.001. Train acc: 0.928. Train Loss: 0.179\n",
      "Step 800. time since epoch: 3893.346. Train acc: 0.928. Train Loss: 0.179\n",
      "Step 810. time since epoch: 3941.812. Train acc: 0.928. Train Loss: 0.179\n",
      "Step 820. time since epoch: 3990.323. Train acc: 0.928. Train Loss: 0.179\n",
      "Step 830. time since epoch: 4038.862. Train acc: 0.928. Train Loss: 0.178\n",
      "Step 840. time since epoch: 4088.430. Train acc: 0.928. Train Loss: 0.178\n",
      "Step 850. time since epoch: 4138.520. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 860. time since epoch: 4187.830. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 870. time since epoch: 4236.236. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 880. time since epoch: 4284.518. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 890. time since epoch: 4332.951. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 900. time since epoch: 4381.615. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 910. time since epoch: 4430.982. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 920. time since epoch: 4479.811. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 930. time since epoch: 4529.392. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 940. time since epoch: 4579.071. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 950. time since epoch: 4628.509. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 960. time since epoch: 4677.809. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 970. time since epoch: 4727.543. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 980. time since epoch: 4776.802. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 990. time since epoch: 4826.369. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1000. time since epoch: 4875.582. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1010. time since epoch: 4924.850. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1020. time since epoch: 4974.376. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1030. time since epoch: 5023.772. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1040. time since epoch: 5073.260. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1050. time since epoch: 5121.375. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1060. time since epoch: 5169.879. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1070. time since epoch: 5217.997. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1080. time since epoch: 5266.612. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1090. time since epoch: 5315.733. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1100. time since epoch: 5365.394. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1110. time since epoch: 5414.266. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1120. time since epoch: 5461.994. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1130. time since epoch: 5510.167. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1140. time since epoch: 5557.907. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1150. time since epoch: 5606.527. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1160. time since epoch: 5655.323. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1170. time since epoch: 5704.060. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1180. time since epoch: 5752.253. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1190. time since epoch: 5800.108. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1200. time since epoch: 5848.463. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1210. time since epoch: 5896.843. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1220. time since epoch: 5945.267. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1230. time since epoch: 5993.460. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1240. time since epoch: 6040.934. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1250. time since epoch: 6088.414. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1260. time since epoch: 6135.752. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1270. time since epoch: 6183.254. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1280. time since epoch: 6231.548. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1290. time since epoch: 6279.068. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1300. time since epoch: 6327.002. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1310. time since epoch: 6375.098. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1320. time since epoch: 6423.069. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1330. time since epoch: 6471.182. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1340. time since epoch: 6519.233. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1350. time since epoch: 6567.840. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1360. time since epoch: 6616.017. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1370. time since epoch: 6664.559. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1380. time since epoch: 6713.232. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1390. time since epoch: 6761.578. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1400. time since epoch: 6810.427. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1410. time since epoch: 6858.798. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1420. time since epoch: 6907.600. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1430. time since epoch: 6955.739. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1440. time since epoch: 7003.929. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1450. time since epoch: 7052.594. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1460. time since epoch: 7101.369. Train acc: 0.929. Train Loss: 0.178\n",
      "Step 1470. time since epoch: 7150.230. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1480. time since epoch: 7198.791. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1490. time since epoch: 7247.753. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1500. time since epoch: 7296.734. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1510. time since epoch: 7345.395. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1520. time since epoch: 7393.664. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1530. time since epoch: 7442.975. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1540. time since epoch: 7492.271. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1550. time since epoch: 7541.263. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1560. time since epoch: 7590.563. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1570. time since epoch: 7640.288. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1580. time since epoch: 7689.585. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1590. time since epoch: 7738.781. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1600. time since epoch: 7789.374. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1610. time since epoch: 7838.468. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1620. time since epoch: 7888.032. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1630. time since epoch: 7937.550. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1640. time since epoch: 7987.324. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1650. time since epoch: 8036.560. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1660. time since epoch: 8085.763. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1670. time since epoch: 8135.422. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1680. time since epoch: 8184.942. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1690. time since epoch: 8234.425. Train acc: 0.929. Train Loss: 0.177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1700. time since epoch: 8283.779. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1710. time since epoch: 8333.299. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1720. time since epoch: 8382.448. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1730. time since epoch: 8431.906. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1740. time since epoch: 8481.334. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1750. time since epoch: 8530.808. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1760. time since epoch: 8579.956. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1770. time since epoch: 8629.586. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1780. time since epoch: 8679.434. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1790. time since epoch: 8728.261. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1800. time since epoch: 8777.458. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1810. time since epoch: 8826.621. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1820. time since epoch: 8875.721. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1830. time since epoch: 8924.593. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1840. time since epoch: 8973.637. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1850. time since epoch: 9023.061. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1860. time since epoch: 9072.224. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1870. time since epoch: 9121.419. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1880. time since epoch: 9170.809. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1890. time since epoch: 9220.365. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1900. time since epoch: 9269.775. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1910. time since epoch: 9318.879. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1920. time since epoch: 9368.562. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1930. time since epoch: 9417.582. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1940. time since epoch: 9466.709. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1950. time since epoch: 9516.202. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1960. time since epoch: 9565.223. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1970. time since epoch: 9614.161. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1980. time since epoch: 9663.385. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 1990. time since epoch: 9712.851. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 2000. time since epoch: 9762.126. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 2010. time since epoch: 9811.361. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 2020. time since epoch: 9859.905. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 2030. time since epoch: 9908.811. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 2040. time since epoch: 9958.130. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 2050. time since epoch: 10007.787. Train acc: 0.929. Train Loss: 0.177\n",
      "Step 2060. time since epoch: 10057.479. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2070. time since epoch: 10107.484. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2080. time since epoch: 10156.323. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2090. time since epoch: 10204.309. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2100. time since epoch: 10252.399. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2110. time since epoch: 10300.399. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2120. time since epoch: 10348.410. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2130. time since epoch: 10396.726. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2140. time since epoch: 10445.037. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2150. time since epoch: 10493.572. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2160. time since epoch: 10541.216. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2170. time since epoch: 10588.853. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2180. time since epoch: 10637.144. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2190. time since epoch: 10685.017. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2200. time since epoch: 10733.702. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2210. time since epoch: 10781.984. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2220. time since epoch: 10830.539. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2230. time since epoch: 10879.073. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2240. time since epoch: 10927.375. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2250. time since epoch: 10975.555. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2260. time since epoch: 11024.704. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2270. time since epoch: 11073.292. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2280. time since epoch: 11122.271. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2290. time since epoch: 11171.243. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2300. time since epoch: 11220.250. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2310. time since epoch: 11269.723. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2320. time since epoch: 11319.294. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2330. time since epoch: 11368.758. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2340. time since epoch: 11418.611. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2350. time since epoch: 11468.420. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2360. time since epoch: 11517.974. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2370. time since epoch: 11567.393. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2380. time since epoch: 11616.891. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2390. time since epoch: 11666.589. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2400. time since epoch: 11716.169. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2410. time since epoch: 11766.200. Train acc: 0.929. Train Loss: 0.176\n",
      "Step 2420. time since epoch: 11816.215. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2430. time since epoch: 11866.012. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2440. time since epoch: 11915.769. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2450. time since epoch: 11965.206. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2460. time since epoch: 12014.813. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2470. time since epoch: 12064.281. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2480. time since epoch: 12114.140. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2490. time since epoch: 12163.700. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2500. time since epoch: 12213.275. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2510. time since epoch: 12263.023. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2520. time since epoch: 12312.968. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2530. time since epoch: 12360.963. Train acc: 0.930. Train Loss: 0.176\n",
      "Step 2540. time since epoch: 12408.888. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2550. time since epoch: 12457.028. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2560. time since epoch: 12505.217. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2570. time since epoch: 12553.431. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2580. time since epoch: 12601.827. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2590. time since epoch: 12651.008. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2600. time since epoch: 12699.723. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2610. time since epoch: 12748.349. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2620. time since epoch: 12797.008. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2630. time since epoch: 12845.804. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2640. time since epoch: 12894.214. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2650. time since epoch: 12942.595. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2660. time since epoch: 12991.255. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2670. time since epoch: 13039.931. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2680. time since epoch: 13088.416. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2690. time since epoch: 13137.362. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2700. time since epoch: 13185.724. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2710. time since epoch: 13234.680. Train acc: 0.930. Train Loss: 0.175\n",
      "Step 2720. time since epoch: 13283.223. Train acc: 0.930. Train Loss: 0.175\n",
      "--------------------\n",
      "epoch 10, loss 0.1751, train acc 0.930, test acc 0.851, time 14229.1 sec\n"
     ]
    }
   ],
   "source": [
    "# Обучение\n",
    "# Очень медленно идет\n",
    "lr, num_epochs  = 0.001, 10\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train(model, train_iter, test_iter, trainer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877aed8",
   "metadata": {},
   "source": [
    "Inception v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Берем точно такую же архитектуру как в лекции, но меняем блоки\n",
    "# замена 3×3 свёртками 1×3 и 3×1 \n",
    "# замена 5×5 свёртками 1×7 и 7×1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf9d537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self, ic, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        self.p1_1 = nn.Sequential(nn.Conv2d(ic, c1, kernel_size=1), nn.ReLU())\n",
    "        \n",
    "        self.p2_1 = nn.Sequential(nn.Conv2d(ic, c2[0], kernel_size=1), nn.ReLU())\n",
    "        self.p2_2 = nn.Sequential(nn.Conv2d(c2[0], c2[1], kernel_size=(1, 3), padding=1), nn.ReLU())\n",
    "        self.p2_3 = nn.Sequential(nn.Conv2d(c2[1], c2[1], kernel_size=(3, 1), padding=1), nn.ReLU())\n",
    "        \n",
    "        self.p3_1 = nn.Sequential(nn.Conv2d(ic, c3[0], kernel_size=1), nn.ReLU())\n",
    "        self.p3_2 = nn.Sequential(nn.Conv2d(c3[0], c3[1], kernel_size=(1, 7), padding=2), nn.ReLU())\n",
    "        self.p3_3 = nn.Sequential(nn.Conv2d(c3[1], c3[1], kernel_size=(7, 1), padding=2), nn.ReLU())\n",
    "        \n",
    "        self.p4_1 = nn.Sequential(nn.MaxPool2d(3, stride=1, padding=1))\n",
    "        self.p4_2 = nn.Sequential(nn.Conv2d(ic, c4, kernel_size=1), nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1_1(x)\n",
    "        p2 = self.p2_3(self.p2_2(self.p2_1(x)))\n",
    "        p3 = self.p3_3(self.p3_2(self.p3_1(x)))\n",
    "        p4 = self.p4_2(self.p4_1(x))\n",
    "\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2599e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.ReLU(),\n",
    "       nn.MaxPool2d(3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25090dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(\n",
    "       nn.Conv2d(64, 64, kernel_size=1),\n",
    "       nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "       nn.MaxPool2d(3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61bb7efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "b3 = nn.Sequential(\n",
    "       Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "       Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "       nn.MaxPool2d(3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6773c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "b4 = nn.Sequential(\n",
    "       Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "       Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "       Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "       Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "       Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "       nn.MaxPool2d(3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ad4d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "b5 = nn.Sequential(\n",
    "       Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "       Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "       nn.AvgPool2d(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c95c82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(b1, b2, b3, b4, b5, nn.Flatten(), nn.Linear(1024, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d9958cad",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 28 but got size 30 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1543\u001b[0m     ):\n",
      "Cell \u001b[1;32mIn[49], line 23\u001b[0m, in \u001b[0;36mInception.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m p3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp3_3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp3_2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp3_1(x)))\n\u001b[0;32m     21\u001b[0m p4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp4_2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp4_1(x))\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp4\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 28 but got size 30 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "summary(net.to(device), input_size=(1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "36e203c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 28 but got size 30 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m lr, num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, train_iter, test_iter, trainer, num_epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m trainer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(y_hat, y)\n\u001b[0;32m     14\u001b[0m l\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1543\u001b[0m     ):\n",
      "Cell \u001b[1;32mIn[49], line 23\u001b[0m, in \u001b[0;36mInception.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m p3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp3_3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp3_2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp3_1(x)))\n\u001b[0;32m     21\u001b[0m p4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp4_2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp4_1(x))\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp4\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 28 but got size 30 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 1\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train(net, train_iter, test_iter, trainer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d565a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame.from_dict(d, orient='index').reset_index()\n",
    "df1.columns = ['эпоха', 'параметры']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f926e",
   "metadata": {},
   "source": [
    "ResNet и DenseNet 161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5126ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# здесь вход должен быть трехканальным\n",
    "transoforms = tv.transforms.Compose([\n",
    "    tv.transforms.Grayscale(3),\n",
    "    tv.transforms.Resize((224, 224)),\n",
    "    tv.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset1 = tv.datasets.EMNIST('.', train = True, transform=transoforms, download=True, split = 'byclass')\n",
    "test_dataset1 = tv.datasets.EMNIST('.', train=False, transform=transoforms, download=True, split = 'byclass')\n",
    "train_iter1 = torch.utils.data.DataLoader(train_dataset1, batch_size=bs)\n",
    "test_iter1 = torch.utils.data.DataLoader(test_dataset1, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# берем модели с практики, но убираем параметры обученности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a96c0dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n3sm3\\anac\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\n3sm3\\anac\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model1 = tv.models.resnet18(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "681610b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e9dda2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Нужно поменять количество классов\n",
    "model1.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4ce650ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fc = nn.Linear(in_features=512, out_features=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fbc4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пробуем также ее обучать\n",
    "lr, num_epochs = 0.001, 10\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train(model1, train_iter1, test_iter1, trainer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame.from_dict(d, orient='index').reset_index()\n",
    "df2.columns = ['эпоха', 'параметры']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "336f6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet\n",
    "model2 = tv.models.densenet161(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "23987903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (features): Sequential(\n",
       "    (conv0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (norm0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (denseblock1): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(144, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition1): _Transition(\n",
       "      (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock2): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(432, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(528, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(624, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(720, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition2): _Transition(\n",
       "      (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock3): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(432, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(528, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(624, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(720, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(816, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(864, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(912, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(912, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(960, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1008, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1104, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1200, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1248, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1296, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1296, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1344, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1392, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1440, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1488, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer25): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1536, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer26): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1584, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1584, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer27): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1632, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer28): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1680, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer29): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1728, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer30): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1776, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1776, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer31): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1824, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer32): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1872, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1872, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer33): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1920, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer34): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1968, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1968, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer35): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(2016, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer36): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(2064, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(2064, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition3): _Transition(\n",
       "      (norm): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(2112, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock4): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1104, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1200, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1248, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1296, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1296, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1344, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1392, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1440, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1488, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1488, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1536, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1584, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1584, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1632, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1680, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1728, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1776, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1776, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1824, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1872, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1872, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1920, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1968, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1968, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(2016, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(2064, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(2064, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(2112, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(2160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(2160, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm5): BatchNorm2d(2208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=2208, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d445cc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2208, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Нужно поменять количество классов\n",
    "model2.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fa899d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.classifier = nn.Linear(in_features=2208, out_features=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "318f4d78",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m lr, num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, train_iter, test_iter, trainer, num_epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m trainer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(y_hat, y)\n\u001b[0;32m     14\u001b[0m l\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torchvision\\models\\densenet.py:213\u001b[0m, in \u001b[0;36mDenseNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 213\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(features, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    215\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(out, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anac\\lib\\site-packages\\torch\\nn\\functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Пробуем с также ее обучать\n",
    "lr, num_epochs = 0.001, 10\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train(model2, train_iter1, test_iter1, trainer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab33d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame.from_dict(d, orient='index').reset_index()\n",
    "df3.columns = ['эпоха', 'параметры']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb9e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e06ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086a2855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff8f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ab68d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
